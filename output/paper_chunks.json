[
  {
    "id": 1,
    "content": "[1] 1 1 [1] 1 1 [1] 1 1 [2] 1 [2] 1 [2] 1 [c] 1.5ex 2 [1] [1] 1 > X > X > m 0.09 > m 0.1 > m 0.08 > m 0.06 Corollary Theorem [theorem] Proposition Example Remark Definition [1] ) [1] ) [1] ) [1] ) [1] ) this link . @makefntext[1] 1 World models, which aim to simulate the real world, have long posed significant challenges in artificial intelligence, influencing various applications, such as robotics, autonomous driving, and gaming. While the specific capabilities required for world models are numerous and not yet precisely defined, approaches such as 3D generation~, 3D/4D scene generation, and video generation demonstrate one or more relevant abilities, such as motion dynamics, interaction and controllability, visual quality, 3D consistency, and generation efficiency. Consequently, these approaches have been adopted in recent works as potential pathways toward realizing world models. While these approaches each excel in partial capabilities toward world modeling, video generation offers"
  },
  {
    "id": 2,
    "content": "e these approaches each excel in partial capabilities toward world modeling, video generation offers one direct and comprehensive pathway, which may be a promising tool for building a world model. From a cognitive science perspective, vision is the dominant sensory modality through which both humans and embodied agents perceive, learn, and reason about the world. Visual streams not only convey spatial layout and object properties but also encode temporal dynamics and causal relationships crucial for prediction and planning. Even complex 3D or 4D simulations can be rendered into videos or images for interpretation, meaning that any human or embodied agent grounds its understanding in visual sequences. This intrinsic reliance on visual representation makes video generation a uniquely natural and information-rich foundation for constructing world models. Recent advances in end-to-end video generation indicate that such models can now serve as high-quality visual renderers, enabling the ex"
  },
  {
    "id": 3,
    "content": "generation indicate that such models can now serve as high-quality visual renderers, enabling the exploration of video-based approaches to world modeling. Recent advancements in techniques, such as diffusion models~ and autoregressive transformers~, have made it possible to generate high-quality videos grounded in fundamental world knowledge. Current methods~ based on these backbones are now capable of producing long-duration, high-quality videos with superficial faithfulness, endowing them with the ability to simulate real-world environments with high fidelity and incorporate multi-modal conditioning. As a result, there is an increasingly evident trend of utilizing video generation models as world models. In this survey, we define a physical world model as a sophisticated digital engine that encodes comprehensive world knowledge to simulate real‑world dynamics in accordance with intrinsic physical and mathematical laws. Such models serve both as high‑fidelity simulators for advancing"
  },
  {
    "id": 4,
    "content": "ic physical and mathematical laws. Such models serve both as high‑fidelity simulators for advancing domains like robotics, autonomous driving, and gaming, and as controlled testbeds for training and evaluating intelligent agents under realistic yet safe conditions. By modeling the physical world, they can also support physics-based engineering, high-stakes decision-making, and other real-world tasks. Recent breakthroughs~ in video generation models, driven by improvements in diffusion models~, autoregressive backbones~, variational autoencoders~, image generation techniques~, controllable image generation ~, and enhancements in training or inference efficiency~, as well as more flexible condition injection modules~, and advances in video rendering~, mark a pivotal moment for the field. These models can now generate high-quality videos grounded in fundamental world knowledge. At this stage, they begin to play a central role in constructing world models. Moreover, with the rapid advancem"
  },
  {
    "id": 5,
    "content": "e, they begin to play a central role in constructing world models. Moreover, with the rapid advancement of technologies such as virtual reality (VR) and embodied AI, the integration of world models into interactive, real-time environments has become increasingly feasible. These technological trends, together with the maturation of video generation methods, indicate that we are on the cusp of a new era where world models will play a central role in shaping autonomous systems, intelligent agents, and immersive virtual environments. This trend is further evidenced in Figure~, which illustrates the dynamics of research attention over recent years. Specifically, while discussions and mentions of world models have been present since 2018, they have remained relatively steady in terms of annual publication volume. While starting in 2024, video generation witnessed an explosive surge in both technical advances and the number of related works, which in turn catalyzed a renewed wave of progress"
  },
  {
    "id": 6,
    "content": "hnical advances and the number of related works, which in turn catalyzed a renewed wave of progress in world models. This pattern highlights a clear interdependence: the rapid advances in video generation are not merely parallel developments but are becoming key enablers that support and accelerate the evolution of world models. Consequently, the present moment is particularly critical for the community to systematically discuss the evolution from video generation to world models, and to chart their future directions as an integrated research frontier. Despite these advances, challenges remain at both the conceptual and structural levels. Conceptually, the definition of a “world model” remains ambiguous, making it difficult to unify perspectives and evaluate progress. Structurally, the field lacks a well-established taxonomy to organize modeling capabilities, developmental stages, and potential trajectories. These gaps highlight an urgent need for a systematic elucidation and comprehen"
  },
  {
    "id": 7,
    "content": "tential trajectories. These gaps highlight an urgent need for a systematic elucidation and comprehensive survey to consolidate existing knowledge and guide the next stage of research. Current surveys~ have laid important groundwork by summarizing related methodologies, datasets, and applications. However, there remains a need to explicitly clarify what aspects have been thoroughly addressed, and which areas, such as real-time integration, controllable video-to-world pipelines, or comprehensive evaluation metrics, are still underexplored. Accordingly, this survey aims to chart a clear path from video generation toward comprehensive world modeling, providing guidance for future research and development in this emerging field. , which collectively determine the evolution of the simulated environment. The model focuses on capturing the causal and spatiotemporal dynamics of the physical environment, while external inputs such as navigation modes or actions act as stimuli that perturb or imp"
  },
  {
    "id": 8,
    "content": "onment, while external inputs such as navigation modes or actions act as stimuli that perturb or impact the environment’s evolution. Hence, the world model is formulated as an interactive environment system that responds to external interventions without explicitly modeling the decision-making process that generates them. Based on this definition, we systematically analyze how video generation models have evolved toward world modeling and propose a four-generation taxonomy according to model capability, as illustrated in Figure~ . We categorize and analyze existing approaches based on their foundation model ability, navigation mode types, application domains, and conditioning strategies. This perspective helps clarify how navigation signals influence video generation and what architectural designs are most effective for different world modeling tasks. Such a clear generational breakdown allows for a systematic evaluation of the progress in world modeling and facilitates a discussion of"
  },
  {
    "id": 9,
    "content": "allows for a systematic evaluation of the progress in world modeling and facilitates a discussion of the remaining challenges and gaps between current video generation systems and ideal world models. We summarize our contributions as follows: have traditionally been regarded as tools enabling AI agents to perceive and interact with their environment, often inspired by human cognition and grounded in so‑called “common sense.” In this survey, we distinguish two complementary perspectives, a physical axis emphasizing external dynamics and a mental axis emphasizing internal simulation and intention modeling, following the conceptual distinction introduced by Huang in his blog post~. From this standpoint, the physical world model represents a more fundamental and global conceptualization, aiming to capture both the evolution and the intrinsic laws of the physical world. In contrast, the mental world model~ can be regarded as a specialized internal cognitive framework that may emerge in high"
  },
  {
    "id": 10,
    "content": "l world model~ can be regarded as a specialized internal cognitive framework that may emerge in higher‑generation physical world models, serving to represent an agent’s internal states, intentions, and preferences. and sim-to-real research, where virtual environments are designed to share semantic and geometric affordances with the real world without being exact replicas. In our context, we define a physical world model as a digital cousin of the real world—one that embeds objective world knowledge and generates observable futures under external conditioning, as illustrated in Figure . Unlike a digital twin that replicates a specific world instance, a digital cousin emphasizes distributional realism, the capacity to simulate diverse, physically plausible yet semantically varied worlds, enabling world models to generalize beyond faithful reproduction. A world model not only simulates the causal dynamics and spatiotemporal evolution of the environment but also encodes the behaviors, inte"
  },
  {
    "id": 11,
    "content": "causal dynamics and spatiotemporal evolution of the environment but also encodes the behaviors, interactions, and goals of agents within it. In practice, the world model is initialized by external inputs, including textual prompts , current observations (images or video clips), and other external interventions that stimulate or guide how the environment evolves, such as audio signals , navigation modes (actions, text commands, or trajectories), and spatial conditions . These inputs define the starting state of the simulation and guide its evolution. Formally, the world mode is defined as follow: IIV 1:T $, which is a stochastic generative process. FS t+1 =F(S t , I t )RV t+1 = R(S t+1 ). F(S t , I t )F(S t+1 | S t , A t )AF(S t+1 , O t+1 | S t , A t )$. This dual interpretation reconciles the apparent contradiction between objectivity and subjectivity: training instills objective physical knowledge, while inference performs subjective reasoning grounded on that learned prior. It also c"
  },
  {
    "id": 12,
    "content": "l knowledge, while inference performs subjective reasoning grounded on that learned prior. It also clarifies that while the world model functions as an objective simulator of latent physics, its operation at inference time is conditioned by subjective, agent-like observations, bridging the two perspectives within a single unified framework. In this section, we provide a brief version of the video generation to world models taxonomy, highlighting the development of core capabilities in each generation. For detailed comparisons, please refer to Section~ and Figure~. We formally define a navigation mode as a structured interface through which an external condition signal guides the generative process. As shown in the Table~, a condition signal is considered a valid instance of navigation mode rather than a spatial condition only if it satisfies a triad of essential properties. We represent this triplet as , capturing the fundamental requirements for temporality, content independence, and"
  },
  {
    "id": 13,
    "content": "this triplet as , capturing the fundamental requirements for temporality, content independence, and spatial reasoning. T - Temporality: The navigation mode must be defined as a temporally ordered sequence or influence the whole duration. This ensures that the guidance signal evolves over time, reflecting realistic changes in intent, observation, or control. R – Content Independence: The navigation mode must not explicitly reference the content and spatial characteristics within the video, including semantic maps, layouts, textual descriptions, motion poses, or depth maps. These types of conditions anchor the generation to specific, interpretable objectives and therefore require pairing with the original video content. They cannot be freely transferred to video generation in other background contexts, which inherently limits their interactive capability. Thus, we define the navigation mode as the content independence type of condition. S – Spatial Reasoning: The navigation mode must sup"
  },
  {
    "id": 14,
    "content": "e as the content independence type of condition. S – Spatial Reasoning: The navigation mode must support spatial reasoning across the generated sequence. This implies that the world model must understand not just static spatial layouts but also dynamic transformations (, agent motion, object displacement) to effectively fulfill the navigation mode. Only when all three criteria are satisfied can a condition be said to activate the navigation mode of a world model. This triadic formulation provides a systematic way to assess whether a video generation model exhibits genuine planning and interactiveness, as opposed to merely replicating appearance or motion patterns. As such, the navigation mode serves as a litmus test for the maturity of world model based generation systems in both controlled and interactive settings. As discussed in Section~, video generation models fundamentally comprise two components: an implicit world model and a video renderer. The world model is responsible for si"
  },
  {
    "id": 15,
    "content": "two components: an implicit world model and a video renderer. The world model is responsible for simulating and predicting the evolution of world states, encapsulating physical laws, causal dynamics, flexible controllability, and real-world planning. The video renderer, in turn, translates the internal states of the world model into visual outputs, specifically in video format, interpretable and accessible to both humans and intelligent agents. This formulation underscores that video generation is not solely about producing realistic visuals; rather, it is about simulating and visualizing coherent world dynamics. As video generation increasingly assumes the role of a world model, its development can be characterized by four major generations. Figure~ outlines the expected levels of faithfulness, interactivity, and planning across these generations, along with the corresponding advancements in capability at each stage. This figure further decomposes these three core dimensions into mor"
  },
  {
    "id": 16,
    "content": "nts in capability at each stage. This figure further decomposes these three core dimensions into more fine-grained second- and third-level sub-capabilities. The characteristics and distinctions of the four generations are elaborated below: Faithfulness is the primary capability emphasized in Generation 1, as it represents the foundational step from video generation towards world models, which shifts from static realism to realistic motion dynamics. This generation is categorized into two stages: the Basic Level and the Advanced Level. The Basic Level consists of traditional and classical approaches, primarily based on GANs and early diffusion models. In contrast, the Advanced Level encompasses video generation models that exhibit superficial faithfulness and low-level interactiveness, while still lacking explicit planning capabilities. As illustrated in Figure~, models in Generation 1 are expected to achieve superficial faithfulness, typically demonstrated by the ability to generate sh"
  },
  {
    "id": 17,
    "content": "e expected to achieve superficial faithfulness, typically demonstrated by the ability to generate short video clips, such as animated video, with tolerable motion distortions. In addition, these models are required to maintain basic video-text consistency, accurately reflecting the primary subject and general background content. However, they often miss specific entities, distort style ordering, and produce unrealistic or incoherent motion patterns at a relatively high frequency. In terms of interactiveness, Generation 1 models exhibit low-level interactiveness, enabling pixel-level interaction and limited controllability. These systems typically support only a single short action composed of a few steps, guided by simple commands such as ``jump,\" ``grab the cup,\" or ``turn left.\" In contrast, they fail to interpret or execute more complex instructions such as ``clean the table,\" ``cook dinner,\" or ``find the key in several drawers.\" Additionally, these models achieve basic condition-t"
  },
  {
    "id": 18,
    "content": "dinner,\" or ``find the key in several drawers.\" Additionally, these models achieve basic condition-to-video consistency, where the generated motion aligns with input conditions to some extent but is often accompanied by subject distortions and incoherent background transitions. While some approaches may incorporate auxiliary conditioning inputs, they primarily rely on spatial modalities, such as sketches, layouts, depth maps, and segmentation masks, which results in limited flexible controllability. Due to the restricted faithfulness and interactiveness in Generation 1, planning capability has not yet emerged at this stage. Interactiveness is the core capability emphasized in Generation 2, termed as semantic and navigational interactiveness, marking a significant advancement in the dimensions of control flexibility, condition-video consistency, subject-centric controllability, and intrinsic model competence. Models in this generation enable flexible control, particularly conditioned on"
  },
  {
    "id": 19,
    "content": "sic model competence. Models in this generation enable flexible control, particularly conditioned on navigation modes, including actions, text commands, and predefined trajectories. They also exhibit emerging basic reasoning abilities, such as inferring motion sequences from high-level commands. In terms of condition-video consistency, Generation 2 models demonstrate notable improvements: they can generate complete and coherent motions with minimal dynamic distortion, and maintain visually appropriate and semantically aligned backgrounds. Another hallmark of this generation is basic subject-centric external control, where the model is capable of interpreting and executing control signals directed at a specific subject, such as instructing one agent to perform a sequence of actions or dynamically adjusting the viewpoint around that subject. However, these models typically operate within relatively static or simplified backgrounds. In addition, the intrinsic model capabilities have advan"
  },
  {
    "id": 20,
    "content": "elatively static or simplified backgrounds. In addition, the intrinsic model capabilities have advanced significantly, as evidenced by the integration of 3D spatial understanding and stronger semantic comprehension rather than basic pixel-level understanding. Together, these advancements position Generation 2 as a transitional phase toward truly interactive and semantically grounded world modeling. Additionally, the long-term vision of interactiveness is building a general-purpose simulator that models the real world and allows human or agent interaction. For example, simulation-based games such as Euro Truck Simulator or SimCity can be viewed as early domain-specific prototypes of interactive world models. Beyond improved interactiveness, Generation 2 world models show significant progress in maintaining temporal coherence and video–text consistency across long and complex sequences. The ability to generate longer videos contributes directly to temporal consistency, ensuring stable ob"
  },
  {
    "id": 21,
    "content": "e ability to generate longer videos contributes directly to temporal consistency, ensuring stable object dynamics and scene layouts over time. Moreover, these models achieve perfect video-text consistency, faithfully rendering all mentioned entities, motions, and events aligned with the given inputs. This generation also begins to capture aspects of the basic physical world, including projective geometry and spatial appropriateness, which contribute to more physically plausible and coherent video generations. Additionally, this generation marks the emergence of simple task planning capabilities. While still limited in scope, these models Planning refers to the model’s ability to simulate the future evolution of a given world state. The broader vision for this level of planning is to faithfully simulate the evolution of the physical world under complex systems, such as weather patterns, narrative plots, cooking processes in robotics, population dynamics, or animal migrations. A vivid im"
  },
  {
    "id": 22,
    "content": "arrative plots, cooking processes in robotics, population dynamics, or animal migrations. A vivid imagination of this capability is portrayed in Liu Cixin’s sci-fi novel The Mirror, where a “super simulator” is capable of projecting the future of the world with arbitrary precision, not merely replaying the past, but modeling the living, ever-changing future. Beyond planning, Generation 3 world models reach physically intrinsic faithfulness, that is, a new pinnacle in physical plausibility, enabling simulations that evolve according to the intrinsic physical principles of the real world. The models are capable of generating arbitrarily long video sequences, which brings higher complexity and enables them to create new motions, entities, viewpoints, and scenes over time, all while maintaining temporal coherence. More impressively, these models internalize the laws of physics themselves. Generation 3 models show evidence of learning causal dynamics across multiple physical domains. Exampl"
  },
  {
    "id": 23,
    "content": "neration 3 models show evidence of learning causal dynamics across multiple physical domains. Examples include rigid-body mechanics (, free-fall, collisions), fluid dynamics, and potentially electromagnetic effects. This represents a fundamental leap: instead of approximating appearances, these models simulate underlying causal processes, leading to greater scientific fidelity and application potential. In terms of interaction, Generation 3 achieves real-time and local interactiveness with high-fidelity controllability, enabling frame-level interaction without perceptible delay. Users can engage with the world model seamlessly, issuing commands and stimuli that lead to instant, coherent changes, whether it's modifying an object's trajectory, switching perspectives, or inserting a new entity into the scene. In addition, local control becomes precise and expressive. These models support subject-centric manipulation with fine-grained attention to contextual and background consistency. For"
  },
  {
    "id": 24,
    "content": "bject-centric manipulation with fine-grained attention to contextual and background consistency. For example, a user can focus on a single character’s behavior while the surrounding environment continues to evolve naturally with rich, photorealistic details, all without compromising visual or physical consistency. Generation 4 world models advance planning capabilities by incorporating stochasticity-aware reasoning, enabling the simulation of both high-probability and low-probability events aligned to the real-world distribution. This supports not only deterministic future prediction but also probabilistic modeling of diverse potential outcomes, especially proactive modeling of black swan events such as earthquakes, tsunamis, financial crises, and asteroid impacts. Furthermore, Generation 4 achieves arbitrary spatial and temporal scale planning. In the spatial domain, the model can plan across macroscopic scales such as universe-level evolution and microscopic scales like microbial dyn"
  },
  {
    "id": 25,
    "content": "across macroscopic scales such as universe-level evolution and microscopic scales like microbial dynamics or atomic-level transitions. Similarly, in the temporal domain, the model is capable of operating across vast time scales, from long-term evolution spanning years or centuries (requiring time compression and critical event selection ability), to mid-scale physical world dynamics, down to fine-grained, high-frequency phenomena such as insect wing beats or human pupil micro-movements. This ability to plan across stochastic events and arbitrary scales represents a critical step toward building general-purpose simulation engines that align more closely with the complexity and uncertainty of the real world. Some recent works~ have begun to explore the mesoscopic-scale planning capabilities of world models, but both the microscopic and macroscopic scales remain largely underexplored. However, these scales are essential for faithfully simulating the physical world, as they capture fine-gr"
  },
  {
    "id": 26,
    "content": "er, these scales are essential for faithfully simulating the physical world, as they capture fine-grained interactions and high-level structural dynamics, respectively. In Generation 4, the capacity for physical faithfulness remains consistent with that of Generation 3. This is because Generation 3 has already achieved a high level of physical realism by accurately adhering to intrinsic physical laws and simulating plausible, causally coherent environments. As such, Generation 4 inherits and maintains this state-of-the-art fidelity, providing a solid and reliable physical foundation upon which more advanced capabilities, such as stochasticity-aware planning and global interactiveness, are built. In addition to its advanced planning capabilities, Generation 4 world models also exhibit a leap in global and multi-modal interactiveness. These models are capable of predicting long-term, multimodal influences resulting from external interventions, allowing for sustained, temporally extended"
  },
  {
    "id": 27,
    "content": "modal influences resulting from external interventions, allowing for sustained, temporally extended interactions across vision, language, and control modalities. At the core of this interactiveness is a form of global control, where an internal agent equipped with a mental world model acts as the primary decision-making entity within the simulated environment. Moreover, these models support multi-entity control by responding to external signals, enabling coordination among multiple agents or systems within the scene. The incorporation of dynamic and evolving backgrounds further enriches the simulation, allowing for more realistic and adaptive world modeling. As shown in Figure~, in the first generation, the requirements for world models focus on the general video generation model capabilities and basic interaction characteristics. Similar to VBench series~, the focus is placed on the superficial faithfulness, including pixel-level frame quality, the duration of the generated video, and"
  },
  {
    "id": 28,
    "content": "erficial faithfulness, including pixel-level frame quality, the duration of the generated video, and video-text consistency. Building upon this foundation, we systematically categorize the existing methods in Generation 1 for the video foundation models~, spatial world models~, and navigated world models~. While video generation models in other domains~ have also made significant progress, this survey focuses specifically on foundation models, pre-trained video generation methods that produce videos based solely on without relying on additional conditioning signals or modalities, due to their potential to support or even serve as comprehensive world models. More importantly, the capabilities of video foundation models largely determine the effectiveness of subsequent adaptations or fine‑tuning for spatial conditions and navigation modes. With their foundational faithfulness and basic interactiveness, these models form the backbone upon which conditional mechanisms (, ControlNet~, Multi"
  },
  {
    "id": 29,
    "content": "eractiveness, these models form the backbone upon which conditional mechanisms (, ControlNet~, Multi-Modal Transformer, Cross-Attention, Concatenation, and Addition) can be applied to achieve high‑quality, physically plausible, and task‑aware video generation. In Generation 1, foundation models demonstrate superficial faithfulness and low-level interactiveness by generating short videos with basic text–video consistency and limited control over simple object motions. They capture key visual details such as object boundaries, textures, and coherent foreground and background layouts. These capabilities support world models in downstream applications like autonomous navigation~ and interactive decision-making~. We summarize representative pre-trained text-to-video (T2V) models~, along with commonly used pre-trained image-to-video (I2V) models~, examining their backbone architectures, video lengths, resolutions, and post-training capabilities related to interactivity and embodiment which a"
  },
  {
    "id": 30,
    "content": "lengths, resolutions, and post-training capabilities related to interactivity and embodiment which are core abilities for serving as world models. This survey also covers both open-sourced models~ and commercial APIs~. For instance, CogVideo~ is widely regarded as an early open-source, large-scale pretrained T2V model that played a pivotal role in initiating the development of video foundation models. The release of Kling 1.0~, built on a DiT backbone, further advanced generative AI toward longer duration and greater visual quality. Following the initial success of Generation 1 models in producing basic text–video consistency and plausible motion dynamics, some research shifted focus toward long and efficient video generation, sometimes at the expense of faithfulness. This led to the development of streaming video generation based on an autoregressive architecture. Specifically, LTX‑Video~ achieves semi‑real‑time generation, continuously producing video clips faster than they can be vi"
  },
  {
    "id": 31,
    "content": "o~ achieves semi‑real‑time generation, continuously producing video clips faster than they can be viewed, by leveraging a high‑compression Video‑VAE and a denoising transformer with full spatiotemporal attention. While notable for its generation speed, its limited fidelity of generated videos still places it within the first generation. However, Generation 1 models lack strong 3D dynamics, making it difficult to maintain physically consistent motion, realistic object interactions, and task-oriented planning ability. As a result, generated videos may contain motion distortions, spatial misalignments, or other unrealistic artifacts that restrict their utility in more advanced interactive or long‑horizon planning tasks. These limitations motivate the developments in Generation 2 foundation models, which aim to enhance visual faithfulness, controllable interaction, and planning ability for more complex world‑modeling scenarios. Although Any2Caption~ introduces the novel paradigm of “any-co"
  },
  {
    "id": 32,
    "content": "ore complex world‑modeling scenarios. Although Any2Caption~ introduces the novel paradigm of “any-condition-to-caption” for video generation, the direct encoding of control signals and their integration with the video generation model remain essential. In particular, adopting a conditioned world model approach, where control signals are explicitly represented and fused within the generation process, provides a more intuitive and effective means of conditional guidance. In this section, we introduce spatial world models with input , which, although exhibiting some interaction capabilities, still suffer from limited video quality and controllability. Representative methods are summarized in Table~ and Table~. In general scenarios, geometry conditioned world models leverage conditional inputs that are semantically or structurally aligned with background content. Typical conditions include sketches, canny edges, depth maps, motion vectors, and human poses, extending the controllability par"
  },
  {
    "id": 33,
    "content": "ketches, canny edges, depth maps, motion vectors, and human poses, extending the controllability paradigm established by ControlNet for image generation to the video domain. Representative works~ support text‑to‑video generation with geometry priors and further extend to other tasks such as animation~, interpolation~ and 3D construction~. Notably, SparseCtrl~ introduces an image reference as geometry guidance for I2V generation, and due to its flexibility, it has been widely adopted as a baseline for controllable I2V/T2V generation under sparse conditioning. 3D prior world models~ utilize 3D point cloud, explicit 3D modeling, or multi‑view imagery to improve spatial consistency in generated videos. While such priors are inherently tied to scene content and thus less adaptable across domains, their strong geometric grounding makes them more effective for producing 3D consistent videos and supporting 3D dynamics. Representative works include Diffusion4D~, which incorporates explicit 3D p"
  },
  {
    "id": 34,
    "content": "supporting 3D dynamics. Representative works include Diffusion4D~, which incorporates explicit 3D point‑cloud priors within a video diffusion framework to achieve spatio‑temporal consistency and high‑fidelity 4D reconstruction, and Zero4D~, which employs explicit reconstructed 3D geometry from a single video in a training‑free manner to guide off‑the‑shelf video diffusion models for rapid 4D generation. Physics prior world models incorporate physical signals or principles into video generation, enabling models to capture and reproduce realistic physical laws. For example, PhysGen~ models forces, torques, and object interactions to simulate classical mechanics phenomena such as collisions governed by Newtonian dynamics. This approach allows generated videos to exhibit motion that is more consistent with real‑world physics, thereby enhancing their plausibility and potential for downstream simulation tasks. Due to the characteristics of the tasks and application scenarios, early autonomo"
  },
  {
    "id": 35,
    "content": "simulation tasks. Due to the characteristics of the tasks and application scenarios, early autonomous driving research featured several spatial conditioned world models compared with robotics and gaming. For example, Delphi~ and Panacea~ employed only scene layouts as spatial conditions to synthesize realistic driving videos, whereas MagicDrive~ combined layouts, bounding boxes, and fixed camera‑pose parameters as a hybrid spatial condition. condition on human pose are also categorize in spatial condition world model. The human pose we mention refers to the skeletal structure of a person in image space, represented as a sequence of keypoints that condition motion or structure within a fixed environment. Although human pose sequences have temporal form, they are spatially bound and depend on scene context, thus classified as spatial conditions rather than navigation conditions. In contrast, navigation conditions, such as trajectories or text commands are scene-independent and allow fre"
  },
  {
    "id": 36,
    "content": "st, navigation conditions, such as trajectories or text commands are scene-independent and allow free navigation across different spatial contexts Navigation world models inviting into the input set as . In general scenes, navigation world models are primarily developed for motion control tasks, including local subject motion~ and camera motion~. These methods often build upon pre-trained video foundation models~, incorporating training-free control modules to enable interactive motion and camera control. However, due to current limitations in video length, generation quality, and the diversity of controllable entities, these approaches remain in the Generation 1 world models. For local motion control tasks, DragAnything~ offers intuitive point‑based dragging to finely adjust subject motion, enabling precise spatial manipulation in generated videos. Peekaboo~ elevates this to semantic‑level control, where object‑centric motion can be directed in line with textual prompts. Pushing furth"
  },
  {
    "id": 37,
    "content": "vel control, where object‑centric motion can be directed in line with textual prompts. Pushing further, Trailblazer~ incorporates trajectory‑driven navigation, supporting complex and continuous motion generation for both subjects and cameras, advancing toward more coherent and context‑aware video navigation. In terms of camera control, existing methods~ inject camera motion trajectories, represented in Plücker coordinates, into pre-trained foundation models via ControlNet-based modules. Alternatively, methods such as GCD~ directly manipulate camera extrinsics, , rotation and translation, as input to control camera viewpoints. Building on the general‑scene setting, navigational world models~ have also been explored in more domain‑specific contexts such as robotics, autonomous driving, and gaming. For instance, in robotics, RoboDreamer~ incorporates additional modalities, such as sketches, to facilitate editable video generation, enabling interaction-level content manipulation. In autono"
  },
  {
    "id": 38,
    "content": "to facilitate editable video generation, enabling interaction-level content manipulation. In autonomous driving, recent works, including ADriver‑I~, DriveDreamer~, Drive‑WM~, MILE~, and GenAD~, further integrate high‑level scene semantics and temporal planning priors to enhance realism, consistency, and task‑oriented controllability. In addition, in virtual gaming, playable video generation~ pioneers interactive, user‑controlled gameplay videos, enabling flexible manipulation of in‑game entities and camera perspectives. Moreover, MarioVGG~ focuses on faithfully recreating game scenes by learning domain‑specific visual dynamics, offering higher visual fidelity and consistency within structured game environments. Compared with Generation 1, Generation 2 world models achieve a marked leap in interactiveness, representing a decisive step toward dynamic and flexible interactive world modeling. Within this generation, interaction can be realized either through spatial conditions (, sketches"
  },
  {
    "id": 39,
    "content": "g. Within this generation, interaction can be realized either through spatial conditions (, sketches, depth, motion pose) or through navigation modes (, trajectory, action, instruction). Spatial conditioned approaches~ excel in producing high‑quality videos with strong 3D dynamics, accurate condition–video consistency, and stable scene interactions, though their flexibility in real‑time control remains limited; nonetheless, these strengths firmly place them within the Generation 2 category. In addition, navigational world models~ offer greater flexibility and transferability, as they do not depend on pre‑existing contextual knowledge and can generalize across diverse entities and scenarios, making them inherently suitable for real‑time and general‑purpose applications. Starting from Generation 2, interactivity and controllability have become essential capabilities for video generation methods serving as world models. These requirements persist and evolve throughout Generation 3 and Gen"
  },
  {
    "id": 40,
    "content": "thods serving as world models. These requirements persist and evolve throughout Generation 3 and Generation 4, with increasingly higher demands placed on control granularity, control modes, modality diversity, real-time responsiveness, and control sensitivity. Given the advancement of controllable video generation models and the critical importance of integrating control signals into foundational video generation models in a principled manner, we systematically mentioned five major condition injection strategies: Condition Injection via ControlNet, Multi-modal Transformer, Cross-Attention, Concatenation, and Addition, with the fundamental architectures shown in Figure~. In this section, we review existing works in Generation 2, structured around world foundation models~ and four representative generative scenarios: general scenes (), robotics (), autonomous driving (), and gaming (). Within each scenario, we organize our discussion by tracing the progression from spatial condition-base"
  },
  {
    "id": 41,
    "content": "hin each scenario, we organize our discussion by tracing the progression from spatial condition-based methods to the more generalizable navigation-mode-based methods, further breaking down each category according to its defining characteristics and modeling approaches. Building upon the first generation, Generation 2 foundation models make a decisive leap in interactiveness, representing a major step toward more dynamic and flexible world modeling. These advances stem from large‑scale pretraining, higher‑capacity architectures, and richer conditioning interfaces, enabling the models to adapt more effectively to downstream control mechanisms. As a result, spatial conditions and navigation‑mode controls can now be integrated with higher video fidelity, stronger spatiotemporal consistency, text-video consistency, greater control flexibility, and better 3D and semantic-level interaction, qualities that directly improve performance in downstream tasks across diverse scenarios. Recent resear"
  },
  {
    "id": 42,
    "content": "lities that directly improve performance in downstream tasks across diverse scenarios. Recent research builds on this momentum, consolidating video generation as the tool for controllable world models. commonly adopt DiT-based~ or some of them leverage~ hybrid architectures that combine diffusion models with causal, frame-by-frame autoregressive mechanisms, achieving a fast, few-step distilled diffusion process to generate each frame. For instance, models such as MAGI‑1~ and CausVid~ extend Diffusion Forcing~ for causal video generation. Self Forcing~ follows this paradigm but removes exposure bias by training with full autoregressive rollout that matches the true inference distribution, enabling efficient video‑level supervision and semi-real‑time generation. And VMoBA~ introduces a mixture-of-block attention mechanism to enhance spatiotemporal representation learning in video diffusion models, achieving improved generation quality and efficiency. In addition, the Cosmos series~ illus"
  },
  {
    "id": 43,
    "content": "models, achieving improved generation quality and efficiency. In addition, the Cosmos series~ illustrates how a powerful foundation model, trained on diverse multimodal inputs, can serve as a digital twin of the physical world while maintaining tight integration with external control, memory, and planning modules. When applied to complex, interactive environments, Cosmos demonstrates consistent, goal‑aligned video generation across navigation and simulation tasks, underscoring how improved interactiveness and condition adaptability in Generation 2 lay the groundwork for more general‑purpose, controllable world models. In addition, Luma AI introduced the Luma - Dream Machine, in which Luma Ray2~ is a next-generation video generative model that integrates ultra-realistic detail, coherent motion, and logical event sequencing, making it particularly powerful in building world models. In this section, we introduce methods that transition from video generation to world modeling in general-p"
  },
  {
    "id": 44,
    "content": "s section, we introduce methods that transition from video generation to world modeling in general-purpose scenarios, with representative methods summarized in Table~. The term general follows its definition in the broader video generation, referring to settings that are not tailored to any specific downstream application. Our discussion focuses on two main groups of methods: (1) Approaches with spatial condition~, which incorporate various priors such as Geometry Conditions, 3D Priors, and Physical Priors. (2) Approaches with navigation mode~, which condition generation on forms of controllable input, including action, trajectory, camera motion, and text instruction. Geometry conditioned world models utilize conditions that are semantically or structurally aligned with the background content. These conditions include, but are not limited to, sketches, canny edges, depth maps, motion vectors, and human poses, inheriting the controllability paradigm introduced by ControlNet~ for image g"
  },
  {
    "id": 45,
    "content": "tors, and human poses, inheriting the controllability paradigm introduced by ControlNet~ for image generation. Compared to Generation 1 approaches, Generation 2 models achieve much finer-grained interaction and better temporal fidelity through sparse, yet semantically rich, conditioning signals. For example, SketchVideo~ enables sketch-based video generation and editing by injecting sparse geometric cues, , keyframe sketches, into a pretrained DiT model, maintaining temporal consistency through inter-frame attention. Compared to Generation 1, it demonstrates significantly improved spatial controllability and interactive editing with minimal user input. 3D prior world models typically aim at 3D or 4D scene reconstruction, leveraging 3D priors to enhance the performance of video generation tasks in construction-oriented settings. In addition to this construction perspective, some approaches~ introduce 3D information specifically to benefit video generation itself. For example, DaS~ lever"
  },
  {
    "id": 46,
    "content": "s~ introduce 3D information specifically to benefit video generation itself. For example, DaS~ leverages 3D point clouds to generate 3D tracking videos of the subject, which are then used as prompts to guide high-quality video synthesis. Physical prior world models~ aim to enhance the physical plausibility of video generation by embedding explicit physical laws and principles into the modeling process. For instance, WISA~ incorporates physical formulas and principles directly as embeddings. PhyT2V~ leverages the physical reasoning capabilities of large language models (LLMs) to guide and improve the physical consistency of video generation models. Camera Motion Navigation World Models~ also rely on trajectories for navigation. Compared to general trajectory navigation, the input trajectories here are derived from camera motion, in the form of explicit camera moving trajectories or implicit reference videos. Moreover, camera movement naturally simulates first-person perspectives of agen"
  },
  {
    "id": 47,
    "content": "it reference videos. Moreover, camera movement naturally simulates first-person perspectives of agents or humans, this has evolved into a distinct and rapidly developing subtask. Therefore, we discuss it separately in this section. For example, CameraCtrl II~ introduces an efficient diffusion framework that supports dynamic video generation guided by explicit camera trajectories, enabling smooth scene exploration through autoregressive clip extension and lightweight camera injection. In contrast, GEN3C further enhances 3D spatial consistency by constructing a global 3D cache from depth predictions, allowing precise camera control and structurally faithful rendering across complex motion paths, advancing the realism and controllability of camera-based navigation models. Instruction Navigation World Models~ utilize text-based instructions, but unlike conventional text prompts that describe background elements, such as scene type, weather, or task themes, these instructions instead guide"
  },
  {
    "id": 48,
    "content": "background elements, such as scene type, weather, or task themes, these instructions instead guide the dynamic aspects of video generation. Specifically, they control motion-related factors of either a single or multiple subjects in the video, or the perspective dynamics of a first-person agent, which can be a human, robot, or camera. For instance, models like Pandora~ and SlowFast-Gen~ can execute directional commands such as “turn left” or “turn right.” In addition, SlowFast-Gen~ supports perspective-based actions such as zooming in and out from a navigational viewpoint. In addition, AETHER~ unifies action-conditioned video prediction and visual planning within a single model from the agent’s first-person view. Remarkably, it also demonstrates strong zero-shot generalization capabilities. Due to the inherently interactive nature of robotics, it has naturally aligned with Generation 2 world models that utilize navigation modes from the very beginning of incorporating video generation"
  },
  {
    "id": 49,
    "content": "world models that utilize navigation modes from the very beginning of incorporating video generation models. Purely spatial conditioned approaches~ are rarely adopted in this domain, as they offer little practical value. In robotics, three primary types of navigation modes are commonly used: action, text instruction, and goal. The action mode typically refers to low-level physical signals such as force and torque applied by robotic arms. The text instruction mode involves simple, short-horizon commands, ``pick up the pen\", while goal navigation encompasses more complex tasks that require long-term planning, such as ``clean up the desk.\" Goal navigation may also be defined by a target image representing the desired end state. For clarity, we refer to short-horizon prompts as text instructions and use the term goal navigation for tasks that require planning over extended temporal horizons, including both textual goals and image-based goal points. Action navigation world models~ typically"
  },
  {
    "id": 50,
    "content": "including both textual goals and image-based goal points. Action navigation world models~ typically involve the prediction of actions as output, or alternatively, leverage video generation models to assist in forecasting the next action. For example, UVA~ and PAD~ employ Transformer architectures to jointly encode video latent features and encoded actions into a unified representation, which is then decoded into both future video frames and an action policy. In the domain of text-guided robotics methods, world models~ at the action planning level have already begun to exhibit characteristics of basic planning capability in Generation 2. Since the introduction of AVDC~, there has been rapid progress in robotics world models capable of goal-directed planning based on image goals or textual instructions. For instance, COMBO~ builds a compositional world model for multi-agent cooperation by factorizing joint actions and generating video predictions to simulate diverse outcomes. In contras"
  },
  {
    "id": 51,
    "content": "factorizing joint actions and generating video predictions to simulate diverse outcomes. In contrast, UniPi~ treats decision-making as a text-conditioned video generation task, where a goal described in natural language is translated into future visual trajectories, from which control actions are extracted. In contrast, methods such as Dreamitate~ adopt a two-stage approach, which first uses a video generation model to produce high-quality future video sequences, which are subsequently utilized as input for action prediction. This separation of video synthesis and policy learning allows the system to benefit from strong visual foresight, enhancing decision-making performance in complex environments. Recently, hybrid navigation methods~ have emerged, enabling models to handle both simple text instructions and image-goal navigation simultaneously. Beyond the text modality, models like UniSim~ and GR-1~ support multi-modal navigation using both text and action inputs to guide video synth"
  },
  {
    "id": 52,
    "content": "Sim~ and GR-1~ support multi-modal navigation using both text and action inputs to guide video synthesis. These multi-control setups raise important questions about balancing robustness and controllability across modalities, as well as potential conflicts between control signals. Notably, UniSim~ supports semi-real-time interaction at the video level, aligning with our vision for advanced forms of Generation 2 world models. Autonomous driving represents another critical application domain for world models. Existing world models in this context are primarily employed to synthesize large-scale, multi-scene datasets or to perform visual and policy prediction tasks, serving as potential solutions to advance autonomous driving toward Level 4 autonomy. In this section, we focus on two major research directions: layout-conditioned world models and those that incorporate multiple navigation modes, including text instructions, trajectory control, action-driven interaction, and hybrid navigation"
  },
  {
    "id": 53,
    "content": "s, including text instructions, trajectory control, action-driven interaction, and hybrid navigation strategies. A detailed comparison of representative methods is provided in Table~. Layout Prior World Models~ in Autonomous Driving primarily refer to models where the input condition encodes the spatial distribution of vehicles and obstacles on the road. This typically includes HD maps, depth information, 3D bounding boxes, and the most common form, BEV (Bird’s-Eye View) layouts. Representative works like UniScene~, which directly synthesizes both videos and LiDAR point clouds from layout-based inputs, facilitating large-scale autonomous driving dataset generation. Additionally, models like DreamForge~ introduce camera poses to better align the layout with the generated video, while Driving Diffusion~ leverages optical flow, effectively serving as an alternative form of layout conditioning. However, because such layout information often relies on annotations available only in curated d"
  },
  {
    "id": 54,
    "content": "ng. However, because such layout information often relies on annotations available only in curated datasets, these approaches may lack practicality for real-time deployment. Accurate and timely collection of layout annotations in real-world applications remains challenging. Therefore, we consider layout-based methods to be an early-stage bridge toward building full-fledged world models in the autonomous driving domain. In autonomous driving scenarios, text instruction world models~ focus on ego-centric viewpoint changes, such as executing a right-turn command. However, existing models~ mainly target video generation conditioned on single-step instructions. Their performance in handling multi-step commands or long-horizon navigation plans remains largely underexplored, particularly in terms of stability and robustness. Trajectory Navigation World Models~ navigate the video generation process by conditioning on either the ego-vehicle trajectory~ or the trajectories of surrounding vehicle"
  },
  {
    "id": 55,
    "content": "ess by conditioning on either the ego-vehicle trajectory~ or the trajectories of surrounding vehicles~. Most of these approaches~ inject trajectory information via trajectory-encoded sequences embedded through Transformers. Others, like GEM~, utilize trainable LoRA modules~ to incorporate trajectory-based navigation in a more flexible and adaptive manner. In Action Navigation World Models~, the term action refers to vehicle control signals such as steering and velocity. DrivingGPT~ adopts a GPT-style autoregressive framework to achieve action-navigated video prediction. Other models~ start from an initial state derived from a combination of textual descriptions and reference images, and then generate future video frames conditioned on action signals. GAIA-1~ represents an interactive autonomous driving world model that casts world modeling as an unsupervised sequence modeling problem, mapping multimodal inputs into discrete tokens and predicting the next token in the sequence to simula"
  },
  {
    "id": 56,
    "content": "pping multimodal inputs into discrete tokens and predicting the next token in the sequence to simulate future scenarios. With the rapid advancement of autonomous driving, there is a growing trend toward the development of unified navigation world models in this domain. Recent works~ have begun to support multiple conditioning signals and navigation modes simultaneously, , combining text instructions with action signals, text with trajectories, or even integrating all four modalities: text instructions, trajectories, action commands, and goal point images. This convergence indicates that autonomous driving world models are gradually evolving toward planning capabilities, aligning with the characteristics of third-generation world models as defined in this paper. However, despite these advances, real-time interaction remains largely absent from current autonomous driving world models. This limitation significantly hinders their practical application as decision-making or assistive module"
  },
  {
    "id": 57,
    "content": "limitation significantly hinders their practical application as decision-making or assistive modules in real-world autonomous vehicles. Among the three primary application domains, even within general scene world modeling, the gaming domain has witnessed the fastest and most mature development of world models. This is largely driven by the inherent demand for real-time interaction in games, which has led most existing models in this area to support real-time feedback and responsiveness. In particular, open-world video generation in games closely aligns with our vision of third- and even fourth-generation world models: constructing a powerful representation model embedded with world knowledge, and then leveraging a video renderer to observe the world from various perspectives and initial conditions. In this setting, the open-world game environment effectively functions as an observable virtual world, one that is both dynamic and interactive in real time. Table~ shows the details of rep"
  },
  {
    "id": 58,
    "content": "irtual world, one that is both dynamic and interactive in real time. Table~ shows the details of representative works in gaming with their navigation modes and control level. However, despite this progress, current game-oriented world models, such as those discussed in~, still face significant limitations in planning capabilities, viewpoint generalization, physical consistency, and semantic world representation. Therefore, we regard them as advanced second-generation world models, with ample room for further advancement. In the gaming domain, key control represents a crucial navigation mode for world models. This includes both inputs from game controllers~ and keyboard-based~ interactions. For instance, models~ rely on controller inputs for gameplay interaction, while GameGen-X~ utilizes keyboard keystrokes to achieve video-level control. Other models~ adopt a combination of keyboard and mouse inputs to enable frame-level control over game environments. Naturally, the complexity and gr"
  },
  {
    "id": 59,
    "content": "mouse inputs to enable frame-level control over game environments. Naturally, the complexity and granularity of supported actions vary across these models. Notably, Google's Genie series~ marks a significant advancement in this area. Genie~ is the first generative interactive environment trained in an unsupervised manner using unlabelled Internet videos. It can synthesize videos frame by frame from diverse inputs, including text-to-image prompts, hand-drawn sketches, text descriptions, or even real-world photos, all of which can be modulated via real-time action inputs. Its successor, Genie 2~, serves as a large-scale foundation world model. It demonstrates emergent capabilities at scale, such as object manipulation, complex character animation, physical reasoning, and even the prediction of other agents’ behavior. The Genie series is thus progressively evolving toward the vision of an interactive open-world simulation model that underpins next-generation world modeling. , which enabl"
  },
  {
    "id": 60,
    "content": "interactive open-world simulation model that underpins next-generation world modeling. , which enables robots to imagine trajectories in unknown environments from a single input image, effectively performing self-navigated, belief-driven video prediction where the input image defines the initial world state. Similarly, Meta’s V-JEPA 2~ predicts the evolution of physical scenes and supports zero-shot robot planning in unseen environments, demonstrating the integration of predictive physical knowledge with adaptive task reasoning. Genie 3~ further exemplifies this generation’s capabilities, achieving real-time (24 FPS, 720 p) interaction with minute-scale visual memory, allowing for consistent scene dynamics even as objects move out of view. It also supports promptable world events, enabling users or agents to modify environmental conditions such as weather or layout mid-simulation, which shows an early form of interactive agent–environment co-evolution. While the third generation of wor"
  },
  {
    "id": 61,
    "content": "shows an early form of interactive agent–environment co-evolution. While the third generation of world models primarily focuses on faithfully simulating regular and rule-governed aspects of the physical world, they tend to favor high-probability trajectories or events during planning. However, the real world is inherently stochastic, characterized not only by deterministic rules but also by the occurrence of rare, unpredictable, and sometimes disruptive events. The fourth generation of world models aims to incorporate this essential dimension of realism by enabling the modeling and simulation of low-probability, outlier events. These rare events, though statistically infrequent, often play a disproportionately significant role in the evolution of complex systems. Examples include genetic mutations leading to pathological outcomes such as cancer, or sudden accidents and coincidences in everyday life, such as traffic collisions. A mature world model should thus strike a principled balanc"
  },
  {
    "id": 62,
    "content": "eryday life, such as traffic collisions. A mature world model should thus strike a principled balance between simulating the most likely futures and accounting for the possibility of unexpected deviations. This necessitates a more probabilistic, uncertainty-aware formulation of planning, one that acknowledges the diversity of real-world dynamics beyond the average case. By embedding stochasticity into world modeling, Generation 4 moves closer to emulating the richness and unpredictability of the physical world, laying the foundation for agents capable of robust decision-making under uncertainty. will likely coincide with the development of higher-level agent intelligence, as richer sensory feedback enables more adaptive and context-aware interaction within stochastic environments. The fourth generation of world models consists of two progressive stages. In the basic stage, the model is capable of learning the probability distributions of real-world events, including the spontaneous gen"
  },
  {
    "id": 63,
    "content": "apable of learning the probability distributions of real-world events, including the spontaneous generation of everyday low-probability scenarios such as traffic accidents, rainy or snowy weather, and balanced stochastic outcomes like coin flips, newborn gender ratios, or vehicle turning directions at intersections. Built upon this probabilistic foundation, the model is further able to simulate extremely rare but high-impact events and generate coherent long-term evolutions conditioned on these events. Examples include financial crises, volcanic eruptions, genetic mutations, or asteroid collisions, events that, while statistically negligible, can fundamentally reshape future trajectories. While these efforts largely operate within the realm of human-scale physics, , mesoscopic spatiotemporal modeling, we envision the advanced stage of Generation 4 models to extend planning capabilities toward both macroscopic and microscopic scales. At the macroscopic scale, a world model should be abl"
  },
  {
    "id": 64,
    "content": "oward both macroscopic and microscopic scales. At the macroscopic scale, a world model should be able to simulate and plan over long-term horizons, , forecasting the development of a system over ten years given an initial state. Importantly, such planning is not expected to occur in real time; rather, the model must compress these ten years into manageable durations (, one hour or even ten minutes), requiring multi-scale temporal modeling. Instead of predicting every single frame or timestep, the model should focus on summarizing salient events. While some existing works begin to address long-horizon forecasting and event-keyframe distillation, progress remains nascent. On the microscopic scale, tasks such as modeling biological phenomena (, involuntary eye microsaccades) remain beyond the reach of current video generation models due to limited temporal resolution and a lack of reasoning capabilities for such fine-grained fluctuations. Yet, this level of precision is crucial for improv"
  },
  {
    "id": 65,
    "content": "capabilities for such fine-grained fluctuations. Yet, this level of precision is crucial for improving both video generation quality and physical realism, and for faithfully simulating real-world subtleties. Most critically, this domain is still vastly underexplored in current literature, presenting a significant opportunity for future research. Beyond Generation 4, we envision a more advanced form of world models that are capable of simulating everything occurring at every time and everywhere. Starting from a given initial state, repeated stochastic rollouts of the model would yield diverse plausible outcomes, resembling the conceptual structure of parallel universes. In this ultimate form, world models could provide downstream agents with virtually unlimited training trajectories and interaction scenarios, compensating for the lack of training data in downstream task domains. with both vision and audio modalities indicates a clear trajectory toward unified audiovisual world models,"
  },
  {
    "id": 66,
    "content": "h vision and audio modalities indicates a clear trajectory toward unified audiovisual world models, where sound, narration, and perception are jointly simulated with physics-based consistency. Moreover, the current paradigm of world modeling is inherently Earth-centric: both environments and agents are restricted to observations on the Earth, where “world” is implicitly defined as “Earth.” However, the physical laws governing environments beyond our planet may differ significantly, most notably in their underlying physics. If future world models can generalize to arbitrary physical laws through fine-tuning or in a zero-shot manner, this would unlock an entirely new set of downstream tasks, such as cosmic simulation or autonomous satellite testing. Such a capability could profoundly advance human understanding of the broader universe. On the one hand, there is the pursuit of world models as accurate simulators. In this paradigm, the ultimate goal is to maximize fidelity to the real phys"
  },
  {
    "id": 67,
    "content": "as accurate simulators. In this paradigm, the ultimate goal is to maximize fidelity to the real physical world, capturing its dynamics and stochasticity with unparalleled precision. Along this pathway, one might envision the eventual creation of a world model that can pass a “Turing Test for reality”: a system so accurate that its generated simulations are indistinguishable from actual observations of the physical world. Such models would serve as powerful scientific instruments, enabling researchers to validate hypotheses and test interventions in silico before deploying them in the real world. In parallel with the vision and simulation centric trajectory of world modeling, another line of research has evolved within the reinforcement learning and robotics communities, where the focus lies on using learned world dynamics to support decision-making and control. Rather than striving solely for pixel-level realism, these models emphasize predictive internal representations that enable ag"
  },
  {
    "id": 68,
    "content": "y for pixel-level realism, these models emphasize predictive internal representations that enable agents to plan, imagine, and act within latent space. Through learning compact transition dynamics, they allow an embodied system to anticipate outcomes of hypothetical actions, perform long-horizon reasoning, and optimize policies before real-world execution. Notable examples such as PlaNet~, Dreamer Series~, and related latent-dynamics models have demonstrated that predictive simulation and planning can effectively bridge perception and control, providing a complementary perspective to video-generation-based world modeling. Together, these directions illustrate how world modeling can advance along both simulation fidelity and decision-oriented reasoning, converging toward a more unified understanding of environment and agency. On the other hand, we envision world models as engines of world knowledge and generative creativity. In this paradigm, the focus shifts from merely replicating a s"
  },
  {
    "id": 69,
    "content": "knowledge and generative creativity. In this paradigm, the focus shifts from merely replicating a single reality to mastering world knowledge and enabling zero-shot generation of diverse possible world patterns. Such models, starting from a single initial state, could instantiate arbitrary virtual worlds, each governed by its own consistent set of physical or abstract laws. Importantly, multiple inferences from the same initial condition would yield divergent yet plausible outcomes, thereby generating parallel universes. In this sense, world models would empower individuals to `create and shape virtual worlds', not merely observing reality but actively creating new ones. These two development directions, precision simulators versus creative generative engines, represent contrasting yet complementary visions. Together, they highlight the profound transformative potential of world models: both as tools for faithfully understanding our universe, and as platforms for exploring the infinit"
  },
  {
    "id": 70,
    "content": "both as tools for faithfully understanding our universe, and as platforms for exploring the infinite possibilities of imagined ones. Building upon the two directions of future world models realized based on the three core capabilities, direct models are poised to profound and potentially disruptive impact human modes of production and daily life, our ways of perceiving and understanding the world, the intellectual level of machine intelligence, and the methodologies employed by researchers across disciplines such as biology~, physics~, astronomy~, medicine~ and chemistry~. Such models have the potential to address many of humanity’s challenges. On one hand, they tackle methodological and technological challenges. For example, in robotics~, an ideal world model could generate infinite real-world interaction data, resolving debates over whether developing better algorithms or collecting larger-scale data is more critical. In autonomous driving~, it would allow us to directly simulate en"
  },
  {
    "id": 71,
    "content": "arger-scale data is more critical. In autonomous driving~, it would allow us to directly simulate endless failure cases, greatly enhancing vehicle safety. On the other hand, world models could address challenges in application domains. For instance, they could predict wildlife~ habitats under varying conditions, monitor microbial growth states, and simulate atmospheric changes, thereby forecasting scenarios in which endangered species may face extinction, identifying protective measures that maximize survival and reproduction, predicting human behaviors that exacerbate global warming and extreme weather, and developing strategies to mitigate the intensifying effects of climate change~. The applications extend even to physics, where such models could simulate multiple possible scenarios for cosmic formation or asteroid impacts on Earth. In conclusion, the evolution of world models promises to reshape the boundaries of human knowledge, creativity, and problem-solving. By integrating accu"
  },
  {
    "id": 72,
    "content": "s to reshape the boundaries of human knowledge, creativity, and problem-solving. By integrating accurate simulation with generative and zero-shot capabilities, these models could serve as both a scientific laboratory and a virtual sandbox, enabling humanity to explore, understand, and intervene in complex systems at unprecedented scales. The pursuit of these dual capabilities represents one of the most ambitious frontiers in artificial intelligence and offers a vision of a future in which humans and machines co-create and navigate multiple possible worlds. * Acknowledgment We would like to thank Jiaming Song for the discussions and valuable feedback."
  }
]