


@inproceedings{nwm,
  title={Navigation world models},
  author={Bar, Amir and Zhou, Gaoyue and Tran, Danny and Darrell, Trevor and LeCun, Yann},
  booktitle={CVPR},
  year={2025}
}

@article{aether,
  title={Aether: Geometric-aware unified world modeling},
  author={Team, Aether and Zhu, Haoyi and Wang, Yifan and Zhou, Jianjun and Chang, Wenzheng and Zhou, Yang and Li, Zizun and Chen, Junyi and Shen, Chunhua and Pang, Jiangmiao and others},
  journal={arXiv preprint arXiv:2503.18945},
  year={2025}
}

@article{worlddreamer,
  title={Worlddreamer: Towards general world models for video generation via predicting masked tokens},
  author={Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Wang, Boyuan and Chen, Xinze and Lu, Jiwen},
  journal={arXiv preprint arXiv:2401.09985},
  year={2024}
}

@article{das,
  title={Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control},
  author={Gu, Zekai and Yan, Rui and Lu, Jiahao and Li, Peng and Dou, Zhiyang and Si, Chenyang and Dong, Zhen and Liu, Qifeng and Lin, Cheng and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2501.03847},
  year={2025}
}

@inproceedings{vdm,
  title={Video diffusion models},
  author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
  booktitle={NeurIPS},
  year={2022}
}


@article{matrix,
  title={The matrix: Infinite-horizon world generation with real-time moving control},
  author={Feng, Ruili and Zhang, Han and Yang, Zhantao and Xiao, Jie and Shu, Zhilei and Liu, Zhiheng and Zheng, Andy and Huang, Yukun and Liu, Yu and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2412.03568},
  year={2024}
}


@inproceedings{edm22,
  title={Elucidating the design space of diffusion-based generative models},
  author={Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{sketchvideo,
  title={SketchVideo: Sketch-based Video Generation and Editing},
  author={Liu, Feng-Lin and Fu, Hongbo and Wang, Xintao and Ye, Weicai and Wan, Pengfei and Zhang, Di and Gao, Lin},
  booktitle={CVPR},
  year={2025}
}

@article{zhang2024moonshot,
  title={Moonshot: Towards controllable video generation and editing with multimodal conditions},
  author={Zhang, David Junhao and Li, Dongxu and Le, Hung and Shou, Mike Zheng and Xiong, Caiming and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2401.01827},
  year={2024}
}

@inproceedings{guo2024sparsectrl,
  title={Sparsectrl: Adding sparse controls to text-to-video diffusion models},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  booktitle={ECCV},
  year={2024}
}

@article{zhang2023controlvideo,
  title={Controlvideo: Training-free controllable text-to-video generation},
  author={Zhang, Yabo and Wei, Yuxiang and Jiang, Dongsheng and Zhang, Xiaopeng and Zuo, Wangmeng and Tian, Qi},
  journal={arXiv preprint arXiv:2305.13077},
  year={2023}
}

@inproceedings{peng2024conditionvideo,
  title={ConditionVideo: training-free condition-guided video generation},
  author={Peng, Bo and Chen, Xinyuan and Wang, Yaohui and Lu, Chaochao and Qiao, Yu},
  booktitle={AAAI},
  year={2024}
}

@inproceedings{khachatryan2023text2video,
  title={Text2video-zero: Text-to-image diffusion models are zero-shot video generators},
  author={Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{wang2023videocomposer,
  title={Videocomposer: Compositional video synthesis with motion controllability},
  author={Wang, Xiang and Yuan, Hangjie and Zhang, Shiwei and Chen, Dayou and Wang, Jiuniu and Zhang, Yingya and Shen, Yujun and Zhao, Deli and Zhou, Jingren},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{zhang2023controlnet,
  title={Adding conditional control to text-to-image diffusion models},
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  booktitle={ICCV},
  year={2023}
}

@article{wang2025cinemaster,
  title={CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation},
  author={Wang, Qinghe and Luo, Yawen and Shi, Xiaoyu and Jia, Xu and Lu, Huchuan and Xue, Tianfan and Wang, Xintao and Wan, Pengfei and Zhang, Di and Gai, Kun},
  journal={arXiv preprint arXiv:2502.08639},
  year={2025}
}

@article{gu2025das,
  title={Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control},
  author={Gu, Zekai and Yan, Rui and Lu, Jiahao and Li, Peng and Dou, Zhiyang and Si, Chenyang and Dong, Zhen and Liu, Qifeng and Lin, Cheng and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2501.03847},
  year={2025}
}

@article{bian2025gsdit,
  title={GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking},
  author={Bian, Weikang and Huang, Zhaoyang and Shi, Xiaoyu and Li, Yijin and Wang, Fu-Yun and Li, Hongsheng},
  journal={arXiv preprint arXiv:2501.02690},
  year={2025}
}

@inproceedings{liang2024diffusion4d,
  title={Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models},
  author={Liang, Hanwen and Yin, Yuyang and Xu, Dejia and Liang, Hanxue and Wang, Zhangyang and Plataniotis, Konstantinos N and Zhao, Yao and Wei, Yunchao},
  booktitle={NeurIPS},
  year={2024}
}

@article{chen2024v3d,
  title={V3d: Video diffusion models are effective 3d generators},
  author={Chen, Zilong and Wang, Yikai and Wang, Feng and Wang, Zhengyi and Liu, Huaping},
  journal={arXiv preprint arXiv:2403.06738},
  year={2024}
}

@inproceedings{voleti2024sv3d,
  title={Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion},
  author={Voleti, Vikram and Yao, Chun-Han and Boss, Mark and Letts, Adam and Pankratz, David and Tochilkin, Dmitry and Laforte, Christian and Rombach, Robin and Jampani, Varun},
  booktitle={ECCV},
  year={2024}
}
@article{wang2025wisa,
  title={Wisa: World simulator assistant for physics-aware text-to-video generation},
  author={Wang, Jing and Ma, Ao and Cao, Ke and Zheng, Jun and Zhang, Zhanjie and Feng, Jiasong and Liu, Shanyuan and Ma, Yuhang and Cheng, Bo and Leng, Dawei and others},
  journal={arXiv preprint arXiv:2503.08153},
  year={2025}
}

@inproceedings{li2025pisa,
  title={PISA experiments: Exploring physics post-training for video diffusion models by watching stuff drop},
  author={Li, Chenyu and Michel, Oscar and Pan, Xichen and Liu, Sainan and Roberts, Mike and Xie, Saining},
  booktitle={ICML},
  year={2025}
}

@inproceedings{liu2024physgen,
  title={Physgen: Rigid-body physics-grounded image-to-video generation},
  author={Liu, Shaowei and Ren, Zhongzheng and Gupta, Saurabh and Wang, Shenlong},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{xue2025phyt2v,
  title={Phyt2v: Llm-guided iterative self-refinement for physics-grounded text-to-video generation},
  author={Xue, Qiyao and Yin, Xiangyu and Yang, Boyuan and Gao, Wei},
  booktitle={CVPR},
  year={2025}
}

@article{furuta2024improving,
  title={Improving dynamic object interactions in text-to-video generation with ai feedback},
  author={Furuta, Hiroki and Zen, Heiga and Schuurmans, Dale and Faust, Aleksandra and Matsuo, Yutaka and Liang, Percy and Yang, Sherry},
  journal={arXiv preprint arXiv:2412.02617},
  year={2024}
}

@article{xiang2024pandora,
  title={Pandora: Towards general world model with natural language actions and video states},
  author={Xiang, Jiannan and Liu, Guangyi and Gu, Yi and Gao, Qiyue and Ning, Yuting and Zha, Yuheng and Feng, Zeyu and Tao, Tianhua and Hao, Shibo and Shi, Yemin and others},
  journal={arXiv preprint arXiv:2406.09455},
  year={2024}
}

@inproceedings{hong2024slowfast,
  title={SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation},
  author={Hong, Yining and Liu, Beide and Wu, Maxine and Zhai, Yuanhao and Chang, Kai-Wei and Li, Linjie and Lin, Kevin and Lin, Chung-Ching and Wang, Jianfeng and Yang, Zhengyuan and others},
  booktitle={ICLR},
  year={2025}
}
@inproceedings{burgert2025gowiththeflow,
  title={Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise},
  author={Burgert, Ryan and Xu, Yuancheng and Xian, Wenqi and Pilarski, Oliver and Clausen, Pascal and He, Mingming and Ma, Li and Deng, Yitong and Li, Lingxiao and Mousavi, Mohsen and others},
  booktitle={CVPR},
  year={2025}
}

@inproceedings{geng2025motionprompting,
  title={Motion prompting: Controlling video generation with motion trajectories},
  author={Geng, Daniel and Herrmann, Charles and Hur, Junhwa and Cole, Forrester and Zhang, Serena and Pfaff, Tobias and Lopez-Guevara, Tatiana and Aytar, Yusuf and Rubinstein, Michael and Sun, Chen and others},
  booktitle={CVPR},
  year={2025}
}

@article{namekata2024sgi2v,
  title={Sg-i2v: Self-guided trajectory control in image-to-video generation},
  author={Namekata, Koichi and Bahmani, Sherwin and Wu, Ziyi and Kant, Yash and Gilitschenski, Igor and Lindell, David B},
  journal={arXiv preprint arXiv:2411.04989},
  year={2024}
}

@inproceedings{fu20243dtrajmaster,
  title={3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation},
  author={Fu, Xiao and Liu, Xian and Wang, Xintao and Peng, Sida and Xia, Menghan and Shi, Xiaoyu and Yuan, Ziyang and Wan, Pengfei and Zhang, Di and Lin, Dahua},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{huang2025fine,
  title={Fine-grained controllable video generation via object appearance and context},
  author={Huang, Hsin-Ping and Su, Yu-Chuan and Sun, Deqing and Jiang, Lu and Jia, Xuhui and Zhu, Yukun and Yang, Ming-Hsuan},
  booktitle={WACV},
  year={2025}
}

@article{qiu2024freetraj,
  title={Freetraj: Tuning-free trajectory control in video diffusion models},
  author={Qiu, Haonan and Chen, Zhaoxi and Wang, Zhouxia and He, Yingqing and Xia, Menghan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16863},
  year={2024}
}

@inproceedings{li2025imageconductor,
  title={Image conductor: Precision control for interactive video synthesis},
  author={Li, Yaowei and Wang, Xintao and Zhang, Zhaoyang and Wang, Zhouxia and Yuan, Ziyang and Xie, Liangbin and Shan, Ying and Zou, Yuexian},
  booktitle={AAAI},
  year={2025}
}

@article{zhang2024interactivevideo,
  title={Interactivevideo: User-centric controllable video generation with synergistic multimodal instructions},
  author={Zhang, Yiyuan and Kang, Yuhao and Zhang, Zhixin and Ding, Xiaohan and Zhao, Sanyuan and Yue, Xiangyu},
  journal={arXiv preprint arXiv:2402.03040},
  year={2024}
}

@inproceedings{jain2024peekaboo,
  title={Peekaboo: Interactive video generation via masked-diffusion},
  author={Jain, Yash and Nasery, Anshul and Vineet, Vibhav and Behl, Harkirat},
  booktitle={CVPR},

  year={2024}
}
@inproceedings{wu2024draganything,
  title={Draganything: Motion control for anything using entity representation},
  author={Wu, Weijia and Li, Zhuang and Gu, Yuchao and Zhao, Rui and He, Yefei and Zhang, David Junhao and Shou, Mike Zheng and Li, Yan and Gao, Tingting and Zhang, Di},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{ma2024trailblazer,
  title={Trailblazer: Trajectory control for diffusion-based video generation},
  author={Ma, Wan-Duo Kurt and Lewis, John P and Kleijn, W Bastiaan},
  booktitle={SIGGRAPH},
  year={2024}
}

@inproceedings{wang2024motionctrl,
  title={Motionctrl: A unified and flexible motion controller for video generation},
  author={Wang, Zhouxia and Yuan, Ziyang and Wang, Xintao and Li, Yaowei and Chen, Tianshui and Xia, Menghan and Luo, Ping and Shan, Ying},
  booktitle={SIGGRAPH},
  year={2024}
}

@inproceedings{hou2024camtrol,
  title={Training-free camera control for video generation},
  author={Hou, Chen and Wei, Guoqiang and Zeng, Yan and Chen, Zhibo},
  booktitle={ICLR},
  year={2025}
}
@article{he2025cameractrl2,
  title={CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models},
  author={He, Hao and Yang, Ceyuan and Lin, Shanchuan and Xu, Yinghao and Wei, Meng and Gui, Liangke and Zhao, Qi and Wetzstein, Gordon and Jiang, Lu and Li, Hongsheng},
  journal={arXiv preprint arXiv:2503.10592},
  year={2025}
}

@inproceedings{he2024cameractrl,
  title={Cameractrl: Enabling camera control for text-to-video generation},
  author={He, Hao and Xu, Yinghao and Guo, Yuwei and Wetzstein, Gordon and Dai, Bo and Li, Hongsheng and Yang, Ceyuan},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{bahmani2025ac3d,
  title={Ac3d: Analyzing and improving 3d camera control in video diffusion transformers},
  author={Bahmani, Sherwin and Skorokhodov, Ivan and Qian, Guocheng and Siarohin, Aliaksandr and Menapace, Willi and Tagliasacchi, Andrea and Lindell, David B and Tulyakov, Sergey},
  booktitle={CVPR},
  year={2025}
}

@inproceedings{bahmani2024vd3d,
  title={Vd3d: Taming large video diffusion transformers for 3d camera control},
  author={Bahmani, Sherwin and Skorokhodov, Ivan and Siarohin, Aliaksandr and Menapace, Willi and Qian, Guocheng and Vasilkovsky, Michael and Lee, Hsin-Ying and Wang, Chaoyang and Zou, Jiaxu and Tagliasacchi, Andrea and others},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{li2025tokenmotion,
  title={TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation},
  author={Li, Ruineng and Xing, Daitao and Sun, Huiming and Ha, Yuanzhou and Shen, Jinglin and Ho, Chiuman},
  booktitle={CVPR},
  year={2025}
}

@inproceedings{van2024gcd,
  title={Generative camera dolly: Extreme monocular dynamic novel view synthesis},
  author={Van Hoorick, Basile and Wu, Rundi and Ozguroglu, Ege and Sargent, Kyle and Liu, Ruoshi and Tokmakov, Pavel and Dave, Achal and Zheng, Changxi and Vondrick, Carl},
  booktitle={ECCV},
  year={2024}
}
@inproceedings{kuang2024cvd,
  title={Collaborative video diffusion: Consistent multi-video generation with camera control},
  author={Kuang, Zhengfei and Cai, Shengqu and He, Hao and Xu, Yinghao and Li, Hongsheng and Guibas, Leonidas J and Wetzstein, Gordon},
  booktitle={NeurIPS},
  year={2024}
}
@inproceedings{zhang2025recapture,
  title={Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning},
  author={Zhang, David Junhao and Paiss, Roni and Zada, Shiran and Karnad, Nikhil and Jacobs, David E and Pritch, Yael and Mosseri, Inbar and Shou, Mike Zheng and Wadhwa, Neal and Ruiz, Nataniel},
  booktitle={CVPR},
  year={2025}
}

@article{xu2024cavia,
  title={Cavia: Camera-controllable multi-view video diffusion with view-integrated attention},
  author={Xu, Dejia and Jiang, Yifan and Huang, Chen and Song, Liangchen and Gernoth, Thorsten and Cao, Liangliang and Wang, Zhangyang and Tang, Hao},
  journal={arXiv preprint arXiv:2410.10774},
  year={2024}
}

@article{xu2024camco,
  title={Camco: Camera-controllable 3d-consistent image-to-video generation},
  author={Xu, Dejia and Nie, Weili and Liu, Chao and Liu, Sifei and Kautz, Jan and Wang, Zhangyang and Vahdat, Arash},
  journal={arXiv preprint arXiv:2406.02509},
  year={2024}
}

@article{wang2024worlddreamer,
  title={Worlddreamer: Towards general world models for video generation via predicting masked tokens},
  author={Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Wang, Boyuan and Chen, Xinze and Lu, Jiwen},
  journal={arXiv preprint arXiv:2401.09985},
  year={2024}
}
@inproceedings{hu2024vpp,
  title={Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations},
  author={Hu, Yucheng and Guo, Yanjiang and Wang, Pengchao and Chen, Xiaoyu and Wang, Yen-Jen and Zhang, Jianke and Sreenath, Koushil and Lu, Chaochao and Chen, Jianyu},
  booktitle={ICML},
  year={2025}
}

@inproceedings{liang2024dreamitate,
  title={Dreamitate: Real-world visuomotor policy learning via video generation},
  author={Liang, Junbang and Liu, Ruoshi and Ozguroglu, Ege and Sudhakar, Sruthi and Dave, Achal and Tokmakov, Pavel and Song, Shuran and Vondrick, Carl},
  booktitle={CoRL},
  year={2024}
}

@article{li2025uva,
  title={Unified video action model},
  author={Li, Shuang and Gao, Yihuai and Sadigh, Dorsa and Song, Shuran},
  journal={arXiv preprint arXiv:2503.00200},
  year={2025}
}

@article{zhang2025forediff,
  title={Consistent World Models via Foresight Diffusion},
  author={Zhang, Yu and Guo, Xingzhuo and Xu, Haoran and Long, Mingsheng},
  journal={arXiv preprint arXiv:2505.16474},
  year={2025}
}
@inproceedings{wu2024ivideogpt,
  title={ivideogpt: Interactive videogpts are scalable world models},
  author={Wu, Jialong and Yin, Shaofeng and Feng, Ningya and He, Xu and Li, Dong and Hao, Jianye and Long, Mingsheng},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{guo2024pad,
  title={Prediction with action: Visual policy learning via joint denoising process},
  author={Guo, Yanjiang and Hu, Yucheng and Zhang, Jianke and Wang, Yen-Jen and Chen, Xiaoyu and Lu, Chaochao and Chen, Jianyu},
  booktitle={NeurIPS},
  year={2024}
}

@article{zhu2024irasim,
  title={Irasim: Learning interactive real-robot action simulators},
  author={Zhu, Fangqi and Wu, Hongtao and Guo, Song and Liu, Yuxiao and Cheang, Chilam and Kong, Tao},
  journal={arXiv preprint arXiv:2406.14540},
  year={2024}
}

@article{he2025dws,
  title={Pre-Trained Video Generative Models as World Simulators},
  author={He, Haoran and Zhang, Yang and Lin, Liang and Xu, Zhongwen and Pan, Ling},
  journal={arXiv preprint arXiv:2502.07825},
  year={2025}
}



@article{wang2025hma,
  title={Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression},
  author={Wang, Lirui and Zhao, Kevin and Liu, Chaoqi and Chen, Xinlei},
  journal={arXiv preprint arXiv:2502.04296},
  year={2025}
}

@inproceedings{kaiser2019mbrl,
  title={Model-based reinforcement learning for atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{zhang2024combo,
  title={COMBO: compositional world models for embodied multi-agent cooperation},
  author={Zhang, Hongxin and Wang, Zeyuan and Lyu, Qiushi and Zhang, Zheyuan and Chen, Sunli and Shu, Tianmin and Dariush, Behzad and Lee, Kwonjoon and Du, Yilun and Gan, Chuang},
  booktitle={ICLR},
  year={2025}
}
@inproceedings{qin2025navigatediff,
  title={Navigatediff: Visual predictors are zero-shot navigation assistants},
  author={Qin, Yiran and Sun, Ao and Hong, Yuze and Wang, Benyou and Zhang, Ruimao},
  booktitle={ICRA},
  year={2025}
}

@inproceedings{du2023unipi,
  title={Learning universal policies via text-guided video generation},
  author={Du, Yilun and Yang, Sherry and Dai, Bo and Dai, Hanjun and Nachum, Ofir and Tenenbaum, Josh and Schuurmans, Dale and Abbeel, Pieter},
  booktitle={NeurIPS},
  year={2024}
}



@inproceedings{he2024vpdd,
  title={Learning an actionable discrete diffusion policy via large-scale actionless video pre-training},
  author={He, Haoran and Bai, Chenjia and Pan, Ling and Zhang, Weinan and Zhao, Bin and Li, Xuelong},
  booktitle={NeurIPS},
  year={2024}
}

@article{ko2023avdc,
  title={Learning to act from actionless videos through dense correspondences},
  author={Ko, Po-Chen and Mao, Jiayuan and Du, Yilun and Sun, Shao-Hua and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2310.08576},
  year={2023}
}

@inproceedings{ajay2023hip,
  title={Compositional foundation models for hierarchical planning},
  author={Ajay, Anurag and Han, Seungwook and Du, Yilun and Li, Shuang and Gupta, Abhi and Jaakkola, Tommi and Tenenbaum, Josh and Kaelbling, Leslie and Srivastava, Akash and Agrawal, Pulkit},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{luo2024videotoaction,
  title={Grounding Video Models to Actions through Goal Conditioned Exploration},
  author={Luo, Yunhao and Du, Yilun},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{zhou2024robodreamer,
  title={Robodreamer: Learning compositional world models for robot imagination},
  author={Zhou, Siyuan and Du, Yilun and Chen, Jiaben and Li, Yandong and Yeung, Dit-Yan and Gan, Chuang},
  booktitle={ICML},
  year={2024}
}


@inproceedings{wen2024vidman,
  title={Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation},
  author={Wen, Youpeng and Lin, Junfan and Zhu, Yi and Han, Jianhua and Xu, Hang and Zhao, Shen and Liang, Xiaodan},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{wu2023gr1,
  title={Unleashing large-scale video generative pre-training for visual robot manipulation},
  author={Wu, Hongtao and Jing, Ya and Cheang, Chilam and Chen, Guangzeng and Xu, Jiafeng and Li, Xinghang and Liu, Minghuan and Li, Hang and Kong, Tao},
  booktitle={ICLR},
  year={2024}
}


@inproceedings{yang2023unisim,
  title={Learning interactive real-world simulators},
  author={Yang, Mengjiao and Du, Yilun and Ghasemipour, Kamyar and Tompson, Jonathan and Schuurmans, Dale and Abbeel, Pieter},
  booktitle={ICLR},
  year={2024}
}
@article{wang2025mila,
  title={MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving},
  author={Wang, Haiguang and Liu, Daqi and Xie, Hongwei and Liu, Haisong and Ma, Enhui and Yu, Kaicheng and Wang, Limin and Wang, Bing},
  journal={arXiv preprint arXiv:2503.15875},
  year={2025}
}


@article{yao2024mygo,
  title={MyGo: Consistent and Controllable Multi-View Driving Video Generation with Camera Control},
  author={Yao, Yining and Guo, Xi and Ding, Chenjing and Wu, Wei},
  journal={arXiv preprint arXiv:2409.06189},
  year={2024}
}


@inproceedings{huang2025subjectdrive,
  title={Subjectdrive: Scaling generative data in autonomous driving via subject control},
  author={Huang, Binyuan and Wen, Yuqing and Zhao, Yucheng and Hu, Yaosi and Liu, Yingfei and Jia, Fan and Mao, Weixin and Wang, Tiancai and Zhang, Chi and Chen, Chang Wen and others},
  booktitle={AAAI},
  year={2025}
}

@article{mei2024dreamforge,
  title={Dreamforge: Motion-aware autoregressive video generation for multi-view driving scenes},
  author={Mei, Jianbiao and Hu, Tao and Yang, Xuemeng and Wen, Licheng and Yang, Yu and Wei, Tiantian and Ma, Yukai and Dou, Min and Shi, Botian and Liu, Yong},
  journal={arXiv preprint arXiv:2409.04003},
  year={2024}
}

@article{lu2024cogdriving,
  title={Seeing Beyond Views: Multi-View Driving Scene Video Generation with Holistic Attention},
  author={Lu, Hannan and Wu, Xiaohe and Wang, Shudong and Qin, Xiameng and Zhang, Xinyu and Han, Junyu and Zuo, Wangmeng and Tao, Ji},
  journal={arXiv preprint arXiv:2412.03520},
  year={2024}
}

@article{ma2024delphi,
  title={Unleashing generalization of end-to-end autonomous driving with controllable long video generation},
  author={Ma, Enhui and Zhou, Lijun and Tang, Tao and Zhang, Zhan and Han, Dong and Jiang, Junpeng and Zhan, Kun and Jia, Peng and Lang, Xianpeng and Sun, Haiyang and others},
  journal={arXiv preprint arXiv:2406.01349},
  year={2024}
}

@inproceedings{gao2023magicdrive,
  title={Magicdrive: Street view generation with diverse 3d geometry control},
  author={Gao, Ruiyuan and Chen, Kai and Xie, Enze and Hong, Lanqing and Li, Zhenguo and Yeung, Dit-Yan and Xu, Qiang},
  booktitle={ICLR},
  year={2024}
}

@article{jiang2024dive,
  title={Dive: Dit-based video generation with enhanced control},
  author={Jiang, Junpeng and Hong, Gangyi and Zhou, Lijun and Ma, Enhui and Hu, Hengtong and Zhou, Xia and Xiang, Jie and Liu, Fan and Yu, Kaicheng and Sun, Haiyang and others},
  journal={arXiv preprint arXiv:2409.01595},
  year={2024}
}

@article{wu2024drivescape,
  title={DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation},
  author={Wu, Wei and Guo, Xi and Tang, Weixuan and Huang, Tingxuan and Wang, Chiyu and Chen, Dongyue and Ding, Chenjing},
  journal={arXiv preprint arXiv:2409.05463},
  year={2024}
}

@inproceedings{li2025uniscene,
  title={Uniscene: Unified occupancy-centric driving scene generation},
  author={Li, Bohan and Guo, Jiazhe and Liu, Hongsi and Zou, Yingshuang and Ding, Yikang and Chen, Xiwu and Zhu, Hu and Tan, Feiyang and Zhang, Chi and Wang, Tiancai and others},
  booktitle={CVPR},
  year={2025}
}

@inproceedings{wen2024panacea,
  title={Panacea: Panoramic and controllable video generation for autonomous driving},
  author={Wen, Yuqing and Zhao, Yucheng and Liu, Yingfei and Jia, Fan and Wang, Yanhui and Luo, Chong and Zhang, Chi and Wang, Tiancai and Sun, Xiaoyan and Zhang, Xiangyu},
  booktitle={CVPR},

  year={2024}
}

@article{ji2025cogen,
  title={Cogen: 3d consistent video generation via adaptive conditioning for autonomous driving},
  author={Ji, Yishen and Zhu, Ziyue and Zhu, Zhenxin and Xiong, Kaixin and Lu, Ming and Li, Zhiqi and Zhou, Lijun and Sun, Haiyang and Wang, Bing and Lu, Tong},
  journal={arXiv preprint arXiv:2503.22231},
  year={2025}
}

@inproceedings{li2024drivingdiffusion,
  title={DrivingDiffusion: Layout-Guided Multi-view Driving Scenarios Video Generation with Latent Diffusion Model},
  author={Li, Xiaofan and Zhang, Yifu and Ye, Xiaoqing},
  booktitle={ECCV},

  year={2024}
}

@inproceedings{zhao2025drivedreamer2,
  title={Drivedreamer-2: Llm-enhanced world models for diverse driving video generation},
  author={Zhao, Guosheng and Wang, Xiaofeng and Zhu, Zheng and Chen, Xinze and Huang, Guan and Bao, Xiaoyi and Wang, Xingang},
  booktitle={AAAI},

  year={2025}
}

@article{jia2023adriveri,
  title={Adriver-i: A general world model for autonomous driving},
  author={Jia, Fan and Mao, Weixin and Liu, Yingfei and Zhao, Yucheng and Wen, Yuqing and Zhang, Chi and Zhang, Xiangyu and Wang, Tiancai},
  journal={arXiv preprint arXiv:2311.13549},
  year={2023}
}

@inproceedings{hassan2025gem,
  title={Gem: A generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control},
  author={Hassan, Mariam and Stapf, Sebastian and Rahimi, Ahmad and Rezende, Pedro and Haghighi, Yasaman and Br{\"u}ggemann, David and Katircioglu, Isinsu and Zhang, Lin and Chen, Xiaoran and Saha, Suman and others},
  booktitle={CVPR},
  year={2025}
}

@article{zhu2025eotwm,
  title={Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space},
  author={Zhu, Jian and Jia, Zhengyu and Gao, Tian and Deng, Jiaxin and Li, Shidi and Liu, Fu and Jia, Peng and Lang, Xianpeng and Sun, Xiaolong},
  journal={arXiv preprint arXiv:2503.09215},
  year={2025}
}

@article{gao2024magicdrive3d,
  title={Magicdrive3d: Controllable 3d generation for any-view rendering in street scenes},
  author={Gao, Ruiyuan and Chen, Kai and Li, Zhihao and Hong, Lanqing and Li, Zhenguo and Xu, Qiang},
  journal={arXiv preprint arXiv:2405.14475},
  year={2024}
}

@article{hu2024drivingworld,
  title={DrivingWorld: ConstructingWorld Model for Autonomous Driving via Video GPT},
  author={Hu, Xiaotao and Yin, Wei and Jia, Mingkai and Deng, Junyuan and Guo, Xiaoyang and Zhang, Qian and Long, Xiaoxiao and Tan, Ping},
  journal={arXiv preprint arXiv:2412.19505},
  year={2024}
}

@article{gao2024magicdrivedit,
  title={MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control},
  author={Gao, Ruiyuan and Chen, Kai and Xiao, Bo and Hong, Lanqing and Li, Zhenguo and Xu, Qiang},
  journal={arXiv preprint arXiv:2411.13807},
  year={2024}
}

@article{arai2024actbench,
  title={ACT-Bench: Towards Action Controllable World Models for Autonomous Driving},
  author={Arai, Hidehisa and Ishihara, Keishi and Takahashi, Tsubasa and Yamaguchi, Yu},
  journal={arXiv preprint arXiv:2412.05337},
  year={2024}
}

@article{chen2024drivinggpt,
  title={Drivinggpt: Unifying driving world modeling and planning with multi-modal autoregressive transformers},
  author={Chen, Yuntao and Wang, Yuqi and Zhang, Zhaoxiang},
  journal={arXiv preprint arXiv:2412.18607},
  year={2024}
}


@article{guo2024infinitydrive,
  title={InfinityDrive: Breaking Time Limits in Driving World Models},
  author={Guo, Xi and Ding, Chenjing and Dou, Haoxuan and Zhang, Xin and Tang, Weixuan and Wu, Wei},
  journal={arXiv preprint arXiv:2412.01522},
  year={2024}
}
@inproceedings{wang2024drivewm,
  title={Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving},
  author={Wang, Yuqi and He, Jiawei and Fan, Lue and Li, Hongxin and Chen, Yuntao and Zhang, Zhaoxiang},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{lu2024wovogen,
  title={Wovogen: World volume-aware diffusion for controllable multi-camera driving scene generation},
  author={Lu, Jiachen and Huang, Ze and Yang, Zeyu and Zhang, Jiahui and Zhang, Li},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{wang2024drivedreamer,
  title={DriveDreamer: Towards Real-World-Drive World Models for Autonomous Driving},
  author={Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Chen, Xinze and Zhu, Jiagang and Lu, Jiwen},
  booktitle={ECCV},
  year={2024}
}


@article{hu2023gaia1,
  title={Gaia-1: A generative world model for autonomous driving},
  author={Hu, Anthony and Russell, Lloyd and Yeo, Hudson and Murez, Zak and Fedoseev, George and Kendall, Alex and Shotton, Jamie and Corrado, Gianluca},
  journal={arXiv preprint arXiv:2309.17080},
  year={2023}

}

@inproceedings{pan2022isodream,
  title={Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models},
  author={Pan, Minting and Zhu, Xiangming and Wang, Yunbo and Yang, Xiaokang},
  booktitle={NeurIPS},
  year={2022}
}


@article{gao2024sem,
  title={Enhance sample efficiency and robustness of end-to-end urban autonomous driving via semantic masked world model},
  author={Gao, Zeyu and Mu, Yao and Chen, Chen and Duan, Jingliang and Luo, Ping and Lu, Yanfeng and Li, Shengbo Eben},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  year={2024}
}

@inproceedings{hu2022mile,
  title={Model-based imitation learning for urban driving},
  author={Hu, Anthony and Corrado, Gianluca and Griffiths, Nicolas and Murez, Zachary and Gurau, Corina and Yeo, Hudson and Kendall, Alex and Cipolla, Roberto and Shotton, Jamie},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{kim2021drivegan,
  title={Drivegan: Towards a controllable high-quality neural simulation},
  author={Kim, Seung Wook and Philion, Jonah and Torralba, Antonio and Fidler, Sanja},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{ni2025maskgwm,
  title={Maskgwm: A generalizable driving world model with video mask reconstruction},
  author={Ni, Jingcheng and Guo, Yuxin and Liu, Yichen and Chen, Rui and Lu, Lewei and Wu, Zehuan},
  booktitle={CVPR},
  year={2025}
}

@inproceedings{gao2024vista,
  title={Vista: A generalizable driving world model with high fidelity and versatile controllability},
  author={Gao, Shenyuan and Yang, Jiazhi and Chen, Li and Chitta, Kashyap and Qiu, Yihang and Geiger, Andreas and Zhang, Jun and Li, Hongyang},
  booktitle={NeurIPS},
  year={2024}
}


@inproceedings{yang2024genad,
  title={Generalized predictive model for autonomous driving},
  author={Yang, Jiazhi and Gao, Shenyuan and Qiu, Yihang and Chen, Li and Li, Tianyu and Dai, Bo and Chitta, Kashyap and Wu, Penghao and Zeng, Jia and Luo, Ping and others},
  booktitle={CVPR},
  year={2024}
}

@article{russell2025gaia2,
  title={Gaia-2: A controllable multi-view generative world model for autonomous driving},
  author={Russell, Lloyd and Hu, Anthony and Bertoni, Lorenzo and Fedoseev, George and Shotton, Jamie and Arani, Elahe and Corrado, Gianluca},
  journal={arXiv preprint arXiv:2503.20523},
  year={2025}
}

@article{liang2025unifuture,
  title={Seeing the Future, Perceiving the Future: A unified driving world model for future generation and perception},
  author={Liang, Dingkang and Zhang, Dingyuan and Zhou, Xin and Tu, Sifan and Feng, Tianrui and Li, Xiaofan and Zhang, Yumeng and Du, Mingyang and Tan, Xiao and Bai, Xiang},
  journal={arXiv preprint arXiv:2503.13587},
  year={2025}
}

@article{yang2024drivepysical,
  title={Pysical Informed Driving World Model},
  author={Yang, Zhuoran and Guo, Xi and Ding, Chenjing and Wang, Chiyu and Wu, Wei},
  journal={arXiv preprint arXiv:2412.08410},
  year={2024}
}

@article{xiao2025worldmem,
  title={WORLDMEM: Long-term consistent world simulation with memory},
  author={Xiao, Zeqi and Lan, Yushi and Zhou, Yifan and Ouyang, Wenqi and Yang, Shuai and Zeng, Yanhong and Pan, Xingang},
  journal={arXiv preprint arXiv:2504.12369},
  year={2025}
}

@inproceedings{robine2023twm,
  title={Transformer-based world models are happy with 100k interactions},
  author={Robine, Jan and H{\"o}ftmann, Marc and Uelwer, Tobias and Harmeling, Stefan},
  booktitle={ICLR},
  year={2023}
}


@inproceedings{micheli2022transformers,
  title={Transformers are sample-efficient world models},
  author={Micheli, Vincent and Alonso, Eloi and Fleuret, Fran{\c{c}}ois},
  booktitle={ICLR},
  year={2023}
}

@article{hafner2023dreamerv3,
  title={Mastering diverse domains through world models},
  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2301.04104},
  year={2023}
}
@inproceedings{hafner2020dreamer2,
  title={Mastering atari with discrete world models},
  author={Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  booktitle={ICLR},
  year={2021}
}


@inproceedings{hafner2019dreamer,
  title={Dream to control: Learning behaviors by latent imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{chiappa2017res,
  title={Recurrent environment simulators},
  author={Chiappa, Silvia and Racaniere, S{\'e}bastien and Wierstra, Daan and Mohamed, Shakir},
  booktitle={ICLR},
  year={2017}
}

@article{chen2025maag,
  title={Model as a game: On numerical and spatial consistency for generative games},
  author={Chen, Jingye and Zhao, Yuzhong and Huang, Yupan and Cui, Lei and Dong, Li and Lv, Tengchao and Chen, Qifeng and Wei, Furu},
  journal={arXiv preprint arXiv:2503.21172},
  year={2025}
}

@article{kanervisto2025wham,
  title={World and human action models towards gameplay ideation},
  author={Kanervisto, Anssi and Bignell, Dave and Wen, Linda Yilin and Grayson, Martin and Georgescu, Raluca and Valcarcel Macua, Sergio and Tan, Shan Zheng and Rashid, Tabish and Pearce, Tim and Cao, Yuhan and others},
  journal={Nature},
  year={2025}
}

@inproceedings{bruce2024genie,
  title={Genie: Generative interactive environments},
  author={Bruce, Jake and Dennis, Michael D and Edwards, Ashley and Parker-Holder, Jack and Shi, Yuge and Hughes, Edward and Lai, Matthew and Mavalankar, Aditi and Steigerwald, Richie and Apps, Chris and others},
  booktitle={ICML},
  year={2024}
}

@inproceedings{green2020mariovgg,
  title={Mario level generation from mechanics using scene stitching},
  author={Green, Michael Cerny and Mugrai, Luvneesh and Khalifa, Ahmed and Togelius, Julian},
  booktitle={IEEE CoG},
  year={2020}
}

@article{yang2024playgen,
  title={Playable Game Generation},
  author={Yang, Mingyu and Li, Junyou and Fang, Zhongbin and Chen, Sheng and Yu, Yangbin and Fu, Qiang and Yang, Wei and Ye, Deheng},
  journal={arXiv preprint arXiv:2412.00887},
  year={2024}
}

@article{feng2024matrix,
  title={The matrix: Infinite-horizon world generation with real-time moving control},
  author={Feng, Ruili and Zhang, Han and Yang, Zhantao and Xiao, Jie and Shu, Zhilei and Liu, Zhiheng and Zheng, Andy and Huang, Yukun and Liu, Yu and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2412.03568},
  year={2024}
}
@inproceedings{che2024gamegenx,
  title={Gamegen-x: Interactive open-world game video generation},
  author={Che, Haoxuan and He, Xuanhua and Liu, Quande and Jin, Cheng and Chen, Hao},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{kim2020gamegan,
  title={Learning to simulate dynamic environments with gamegan},
  author={Kim, Seung Wook and Zhou, Yuhao and Philion, Jonah and Torralba, Antonio and Fidler, Sanja},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{menapace2022playableenvironment,
  title={Playable environments: Video manipulation in space and time},
  author={Menapace, Willi and Lathuili{\`e}re, St{\'e}phane and Siarohin, Aliaksandr and Theobalt, Christian and Tulyakov, Sergey and Golyanik, Vladislav and Ricci, Elisa},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{menapace2021pvg,
  title={Playable video generation},
  author={Menapace, Willi and Lathuiliere, Stephane and Tulyakov, Sergey and Siarohin, Aliaksandr and Ricci, Elisa},
  booktitle={CVPR},
  year={2021}
}

@article{guo2025mineworld,
  title={Mineworld: a real-time and open-source interactive world model on minecraft},
  author={Guo, Junliang and Ye, Yang and He, Tianyu and Wu, Haoyu and Jiang, Yushu and Pearce, Tim and Bian, Jiang},
  journal={arXiv preprint arXiv:2504.08388},
  year={2025}
}

@article{yu2025gamefactory,
  title={GameFactory: Creating New Games with Generative Interactive Videos},
  author={Yu, Jiwen and Qin, Yiran and Wang, Xintao and Wan, Pengfei and Zhang, Di and Liu, Xihui},
  journal={arXiv preprint arXiv:2501.08325},
  year={2025}
}

@misc{decart2024oasis,
  author       = {Decart, Etched and McIntyre, Quinn and Campbell, Spruce and Chen, Xinlei and Wachen, Robert},
  title        = {Oasis: A universe in a transformer},
  year         = {2024},
  howpublished = {\url{https://oasis-model.github.io}},
  note         = {Accessed: 2025-06-13}
}



@misc{parker2024genie2,
  author       = {Google Deepmind},
  title        = {Parker-Holder, J and Ball, P and Bruce, J and Dasagi, V and Holsheimer, K and Kaplanis, C and Moufarek, A and Scully, G and Shar, J and Shi, J and others},
  year         = {2024},
  howpublished = {\url{https://deepmind. google/discover/blog/genie-2-a-large-scale-foundation-world-model}},
  note         = {Accessed: 2025-06-13}
}



@inproceedings{valevski2024gamengen,
  title={Diffusion models are real-time game engines},
  author={Valevski, Dani and Leviathan, Yaniv and Arar, Moab and Fruchter, Shlomi},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{alonso2024diamond,
  title={Diffusion for world modeling: Visual details matter in atari},
  author={Alonso, Eloi and Jelley, Adam and Micheli, Vincent and Kanervisto, Anssi and Storkey, Amos J and Pearce, Tim and Fleuret, Fran{\c{c}}ois},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{gao2025adaworld,
  title={Adaworld: Learning adaptable world models with latent actions},
  author={Gao, Shenyuan and Zhou, Siyuan and Du, Yilun and Zhang, Jun and Gan, Chuang},
  booktitle={ICML},
  year={2025}
}

@article{lecun2022path,
  title={A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  year={2022}
}


@article{kang2024far,
  title={How far is video generation from world model: A physical law perspective},
  author={Kang, Bingyi and Yue, Yang and Lu, Rui and Lin, Zhijie and Zhao, Yang and Wang, Kaixin and Huang, Gao and Feng, Jiashi},
  journal={arXiv preprint arXiv:2411.02385},
  year={2024}
}

@article{yu2025position,
  title={Position: Interactive generative video as next-generation game engine},
  author={Yu, Jiwen and Qin, Yiran and Che, Haoxuan and Liu, Quande and Wang, Xintao and Wan, Pengfei and Zhang, Di and Liu, Xihui},
  journal={arXiv preprint arXiv:2503.17359},
  year={2025}
}

@article{ha2018worldmodel,
  title={World models},
  author={Ha, David and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1803.10122},
  year={2018}
}

@article{aldausari2022video,
  title={Video generative adversarial networks: a review},
  author={Aldausari, Nuha and Sowmya, Arcot and Marcus, Nadine and Mohammadi, Gelareh},
  journal={CSUR},
  year={2022}
}

@article{xing2024survey,
  title={A survey on video diffusion models},
  author={Xing, Zhen and Feng, Qijun and Chen, Haoran and Dai, Qi and Hu, Han and Xu, Hang and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={ACM Computing Surveys},
  year={2024}
}

@article{liu2024soraareview,
  title={Sora: A review on background, technology, limitations, and opportunities of large vision models},
  author={Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and others},
  journal={arXiv preprint arXiv:2402.17177},
  year={2024}
}
@article{guan2024adinitialsurvey,
  title={World models for autonomous driving: An initial survey},
  author={Guan, Yanchen and Liao, Haicheng and Li, Zhenning and Hu, Jia and Yuan, Runze and Zhang, Guohui and Xu, Chengzhong},
  journal={IEEE Transactions on Intelligent Vehicles},
  year={2024}
}

@article{cho2024soraasansgisurvey,
  title={Sora as an agi world model? a complete survey on text-to-video generation},
  author={Cho, Joseph and Puspitasari, Fachrina Dewi and Zheng, Sheng and Zheng, Jingyao and Lee, Lik-Hang and Kim, Tae-Ho and Hong, Choong Seon and Zhang, Chaoning},
  journal={arXiv preprint arXiv:2403.05131},
  year={2024}
}

@article{melnik2024videosurvey,
  title={Video diffusion models: A survey},
  author={Melnik, Andrew and Ljubljanac, Michal and Lu, Cong and Yan, Qi and Ren, Weiming and Ritter, Helge},
  journal={arXiv preprint arXiv:2405.03150},
  year={2024}
}

@article{zhu2024issoraasurvey,
  title={Is sora a world simulator? a comprehensive survey on general world models and beyond},
  author={Zhu, Zheng and Wang, Xiaofeng and Zhao, Wangbo and Min, Chen and Deng, Nianchen and Dou, Min and Wang, Yuqi and Shi, Botian and Wang, Kai and Zhang, Chi and others},
  journal={arXiv preprint arXiv:2405.03520},
  year={2024}
}

@article{sun2024fromsorawecanseesurvey,
  title={From sora what we can see: A survey of text-to-video generation},
  author={Sun, Rui and Zhang, Yumin and Shah, Tejal and Sun, Jiahao and Zhang, Shuoying and Li, Wenqi and Duan, Haoran and Wei, Bo and Ranjan, Rajiv},
  journal={arXiv preprint arXiv:2405.10674},
  year={2024}
}

@article{fu2024exploringsurvey,
  title={Exploring the interplay between video generation and world models in autonomous driving: A survey},
  author={Fu, Ao and Zhou, Yi and Zhou, Tao and Yang, Yi and Gao, Bojun and Li, Qun and Wu, Guobin and Shao, Ling},
  journal={arXiv preprint arXiv:2411.02914},
  year={2024}
}

@article{dal2024jointadsurvey,
  title={Joint Perception and Prediction for Autonomous Driving: A Survey},
  author={Dal'Col, Lucas and Oliveira, Miguel and Santos, V{\'\i}tor},
  journal={arXiv preprint arXiv:2412.14088},
  year={2024}
}

@article{liu2025generativesurvey,
  title={Generative physical ai in vision: A survey},
  author={Liu, Daochang and Zhang, Junyu and Dinh, Anh-Dung and Park, Eunbyung and Zhang, Shichao and Mian, Ajmal and Shah, Mubarak and Xu, Chang},
  journal={arXiv preprint arXiv:2501.10928},
  year={2025}
}

@article{lin2025exploringsurvey,
  title={Exploring the evolution of physics cognition in video generation: A survey},
  author={Lin, Minghui and Wang, Xiang and Wang, Yishan and Wang, Shu and Dai, Fengqi and Ding, Pengxiang and Wang, Cunxiang and Zuo, Zhengrong and Sang, Nong and Huang, Siteng and others},
  journal={arXiv preprint arXiv:2503.21765},
  year={2025}
}

@article{wang2025survey,
  title={Survey of Video Diffusion Models: Foundations, Implementations, and Applications},
  author={Wang, Yimu and Liu, Xuye and Pang, Wei and Ma, Li and Yuan, Shuai and Debevec, Paul and Yu, Ning},
  journal={arXiv preprint arXiv:2504.16081},
  year={2025}
}

@article{yu2025surveyigv,
  title={A survey of interactive generative video},
  author={Yu, Jiwen and Qin, Yiran and Che, Haoxuan and Liu, Quande and Wang, Xintao and Wan, Pengfei and Zhang, Di and Gai, Kun and Chen, Hao and Liu, Xihui},
  journal={arXiv preprint arXiv:2504.21853},
  year={2025}
}

@inproceedings{huang2024vbench,
  title={Vbench: Comprehensive benchmark suite for video generative models},
  author={Huang, Ziqi and He, Yinan and Yu, Jiashuo and Zhang, Fan and Si, Chenyang and Jiang, Yuming and Zhang, Yuanhan and Wu, Tianxing and Jin, Qingyang and Chanpaisit, Nattapol and others},
  booktitle={CVPR},
  year={2024}
}

@article{huang2024vbench++,
  title={Vbench++: Comprehensive and versatile benchmark suite for video generative models},
  author={Huang, Ziqi and Zhang, Fan and Xu, Xiaojie and He, Yinan and Yu, Jiashuo and Dong, Ziyue and Ma, Qianli and Chanpaisit, Nattapol and Si, Chenyang and Jiang, Yuming and others},
  journal={arXiv preprint arXiv:2411.13503},
  year={2024}
}

@article{bansal2024videophy,
  title={Videophy: Evaluating physical commonsense for video generation},
  author={Bansal, Hritik and Lin, Zongyu and Xie, Tianyi and Zong, Zeshun and Yarom, Michal and Bitton, Yonatan and Jiang, Chenfanfu and Sun, Yizhou and Chang, Kai-Wei and Grover, Aditya},
  journal={arXiv preprint arXiv:2406.03520},
  year={2024}
}

@article{meng2024towards,
  title={Towards world simulator: Crafting physical commonsense-based benchmark for video generation},
  author={Meng, Fanqing and Liao, Jiaqi and Tan, Xinyu and Shao, Wenqi and Lu, Quanfeng and Zhang, Kaipeng and Cheng, Yu and Li, Dianqi and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2410.05363},
  year={2024}
}

@article{qin2024worldsimbench,
  title={Worldsimbench: Towards video generation models as world simulators},
  author={Qin, Yiran and Shi, Zhelun and Yu, Jiwen and Wang, Xijun and Zhou, Enshen and Li, Lijun and Yin, Zhenfei and Liu, Xihui and Sheng, Lu and Shao, Jing and others},
  journal={arXiv preprint arXiv:2410.18072},
  year={2024}
}

@article{li2025worldmodelbench,
  title={Worldmodelbench: Judging video generation models as world models},
  author={Li, Dacheng and Fang, Yunhao and Chen, Yukang and Yang, Shuo and Cao, Shiyi and Wong, Justin and Luo, Michael and Wang, Xiaolong and Yin, Hongxu and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2502.20694},
  year={2025}
}

@article{bansal2025videophy,
  title={Videophy-2: A challenging action-centric physical commonsense evaluation in video generation},
  author={Bansal, Hritik and Peng, Clark and Bitton, Yonatan and Goldenberg, Roman and Grover, Aditya and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2503.06800},
  year={2025}
}

@article{ju2025fulldit,
  title={Fulldit: Multi-task video generative foundation model with full attention},
  author={Ju, Xuan and Ye, Weicai and Liu, Quande and Wang, Qiulin and Wang, Xintao and Wan, Pengfei and Zhang, Di and Gai, Kun and Xu, Qiang},
  journal={arXiv preprint arXiv:2503.19907},
  year={2025}
}

@article{zheng2025vbench2,
  title={Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness},
  author={Zheng, Dian and Huang, Ziqi and Liu, Hongbo and Zou, Kai and He, Yinan and Zhang, Fan and Zhang, Yuanhan and He, Jingwen and Zheng, Wei-Shi and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2503.21755},
  year={2025}
}

@article{duan2025worldscore,
  title={Worldscore: A unified evaluation benchmark for world generation},
  author={Duan, Haoyi and Yu, Hong-Xing and Chen, Sirui and Fei-Fei, Li and Wu, Jiajun},
  journal={arXiv preprint arXiv:2504.00983},
  year={2025}
}

@inproceedings{micheli2022iris,
  title={Transformers are sample-efficient world models},
  author={Micheli, Vincent and Alonso, Eloi and Fleuret, Fran{\c{c}}ois},
  booktitle={ICLR},
  year={2023}
}

@article{chen2023controlavideo,
  title={Control-A-Video: Controllable Text-to-Video Diffusion Models with Motion Prior and Reward Feedback Learning},
  author={Chen, Weifeng and Ji, Yatai and Wu, Jie and Wu, Hefeng and Xie, Pan and Li, Jiashi and Xia, Xin and Xiao, Xuefeng and Lin, Liang},
  journal={arXiv preprint arXiv:2305.13840},
  year={2023}
}

@article{zuo2024videomv,
  title={Videomv: Consistent multi-view generation based on large video generative model},
  author={Zuo, Qi and Gu, Xiaodong and Qiu, Lingteng and Dong, Yuan and Yuan, Weihao and Peng, Rui and Zhu, Siyu and Bo, Liefeng and Dong, Zilong and Huang, Qixing and others},
  year={2024}
}

@article{zhao2025synthetic,
  title={Synthetic video enhances physical fidelity in video synthesis},
  author={Zhao, Qi and Ni, Xingyu and Wang, Ziyu and Cheng, Feng and Yang, Ziyan and Jiang, Lu and Wang, Bohan},
  journal={arXiv preprint arXiv:2503.20822},
  year={2025}
}

@inproceedings{yang2024directavideo,
  title={Direct-a-video: Customized video generation with user-directed camera movement and object motion},
  author={Yang, Shiyuan and Hou, Liang and Huang, Haibin and Ma, Chongyang and Wan, Pengfei and Zhang, Di and Chen, Xiaodong and Liao, Jing},
  booktitle={SIGGRAPH},
  year={2024}
}


@article{bai2025recammaster,
  title={ReCamMaster: Camera-Controlled Generative Rendering from A Single Video},
  author={Bai, Jianhong and Xia, Menghan and Fu, Xiao and Wang, Xintao and Mu, Lianrui and Cao, Jinwen and Liu, Zuozhu and Hu, Haoji and Bai, Xiang and Wan, Pengfei and others},
  journal={arXiv preprint arXiv:2503.11647},
  year={2025}
}

@article{alhaija2025cosmostransfer1,
  title={Cosmos-transfer1: Conditional world generation with adaptive multimodal control},
  author={Alhaija, Hassan Abu and Alvarez, Jose and Bala, Maciej and Cai, Tiffany and Cao, Tianshi and Cha, Liz and Chen, Joshua and Chen, Mike and Ferroni, Francesco and Fidler, Sanja and others},
  journal={arXiv preprint arXiv:2503.14492},
  year={2025}
}

@article{agarwal2025cosmos,
  title={Cosmos world foundation model platform for physical ai},
  author={Agarwal, Niket and Ali, Arslan and Bala, Maciej and Balaji, Yogesh and Barker, Erik and Cai, Tiffany and Chattopadhyay, Prithvijit and Chen, Yongxin and Cui, Yin and Ding, Yifan and others},
  journal={arXiv preprint arXiv:2501.03575},
  year={2025}
}

@inproceedings{fan2024videoagent,
  title={Videoagent: A memory-augmented multimodal agent for video understanding},
  author={Fan, Yue and Ma, Xiaojian and Wu, Rujie and Du, Yuntao and Li, Jiaqi and Gao, Zhi and Li, Qing},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{yang2024cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  booktitle={ICLR},
  year={2025}
}

@article{hong2022cogvideo,
  title={Cogvideo: Large-scale pretraining for text-to-video generation via transformers},
  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  journal={arXiv preprint arXiv:2205.15868},
  year={2022}
}

@article{guo2023animatediff,
  title={Animatediff: Animate your personalized text-to-image diffusion models without specific tuning},
  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Liang, Zhengyang and Wang, Yaohui and Qiao, Yu and Agrawala, Maneesh and Lin, Dahua and Dai, Bo},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023}
}

@article{podell2023sdxl,
  title={Sdxl: Improving latent diffusion models for high-resolution image synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  journal={arXiv preprint arXiv:2307.01952},
  year={2023}
}


@article{blattmann2023svd,
  title={Stable video diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023}
}
@article{wang2023modelscope,
  title={Modelscope text-to-video technical report},
  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},
  journal={arXiv preprint arXiv:2308.06571},
  year={2023}
}


@article{zhang2023i2vgen,
  title={I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models},
  author={Zhang, Shiwei and Wang, Jiayu and Zhang, Yingya and Zhao, Kang and Yuan, Hangjie and Qin, Zhiwu and Wang, Xiang and Zhao, Deli and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.04145},
  year={2023}
}

@article{zheng2024opensora,
  title={Open-sora: Democratizing efficient video production for all},
  author={Zheng, Zangwei and Peng, Xiangyu and Yang, Tianji and Shen, Chenhui and Li, Shenggui and Liu, Hongxin and Zhou, Yukun and Li, Tianyi and You, Yang},
  journal={arXiv preprint arXiv:2412.20404},
  year={2024}
}

@article{lin2024opensoraplan,
  title={Open-sora plan: Open-source large video generation model},
  author={Lin, Bin and Ge, Yunyang and Cheng, Xinhua and Li, Zongjian and Zhu, Bin and Wang, Shaodong and He, Xianyi and Ye, Yang and Yuan, Shenghai and Chen, Liuhan and others},
  journal={arXiv preprint arXiv:2412.00131},
  year={2024}
}

@article{chen2023videocrafter1,
  title={Videocrafter1: Open diffusion models for high-quality video generation},
  author={Chen, Haoxin and Xia, Menghan and He, Yingqing and Zhang, Yong and Cun, Xiaodong and Yang, Shaoshu and Xing, Jinbo and Liu, Yaofang and Chen, Qifeng and Wang, Xintao and others},
  journal={arXiv preprint arXiv:2310.19512},
  year={2023}
}

@inproceedings{bar2024lumiere,
  title={Lumiere: A space-time diffusion model for video generation},
  author={Bar-Tal, Omer and Chefer, Hila and Tov, Omer and Herrmann, Charles and Paiss, Roni and Zada, Shiran and Ephrat, Ariel and Hur, Junhwa and Liu, Guanghui and Raj, Amit and others},
  booktitle={SIGGRAPH},
  year={2024}
}


@inproceedings{xing2024dynamicrafter,
  title={Dynamicrafter: Animating open-domain images with video diffusion priors},
  author={Xing, Jinbo and Xia, Menghan and Zhang, Yong and Chen, Haoxin and Yu, Wangbo and Liu, Hanyuan and Liu, Gongye and Wang, Xintao and Shan, Ying and Wong, Tien-Tsin},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{rombach2022ldm,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{esser2024sd3,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={ICML},
  year={2024}
}

@inproceedings{dosovitskiy2020vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={ICLR},
  year={2021}
}


@inproceedings{esser2024mmdit,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={ICML},
  year={2024}
}

@inproceedings{chen2023seine,
  title={Seine: Short-to-long video diffusion model for generative transition and prediction},
  author={Chen, Xinyuan and Wang, Yaohui and Zhang, Lingjun and Zhuang, Shaobin and Ma, Xin and Yu, Jiashuo and Wang, Yali and Lin, Dahua and Qiao, Yu and Liu, Ziwei},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{peebles2023dit,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{shridhar2022cliport,
  title={Cliport: What and where pathways for robotic manipulation},
  author={Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle={PMLR},
  year={2022}
}

@inproceedings{brooks2023instructpix2pix,
  title={Instructpix2pix: Learning to follow image editing instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  booktitle={CVPR},
  year={2023}
}


@misc{sora,
  author       = {T. Brooks and B. Peebles and C. Homes and W. DePue and Y. Guo and L. Jing and D. Schnurr and J. Taylor and T. Luhman and E. Luhman and C. W. Y. Ng and R. Wang and A. Ramesh},
  title        = {Video generation models as world simulators},
  year         = {2024},
  howpublished = {\url{https://openai.com/index/video-generation-models-as-world-simulators/}},
  note         = {Accessed: 2024-06-13}
}


@article{wang2025lavie,
  title={Lavie: High-quality video generation with cascaded latent diffusion models},
  author={Wang, Yaohui and Chen, Xinyuan and Ma, Xin and Zhou, Shangchen and Huang, Ziqi and Wang, Yi and Yang, Ceyuan and He, Yinan and Yu, Jiashuo and Yang, Peiqing and others},
  journal={IJCV},
  year={2025}
}

@inproceedings{ren2025gen3c,
  title={GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control},
  author={Ren, Xuanchi and Shen, Tianchang and Huang, Jiahui and Ling, Huan and Lu, Yifan and Nimier-David, Merlin and M\"uller, Thomas and Keller, Alexander and Fidler, Sanja and Gao, Jun},
  booktitle={CVPR},
  year={2025}
}

@misc{runway2024gen3,
  author       = {Runway},
  title        = {Introducing gen-3 alpha: A new frontier for video generation},
  year         = {2024},
  howpublished = {\url{https://runwayml.com/research/introducing-gen-3-alpha}},
  note         = {Accessed: 2025-06-13}
}

@article{teng2025magi,
  title={MAGI-1: Autoregressive Video Generation at Scale},
  author={Teng, Hansi and Jia, Hongyu and Sun, Lei and Li, Lingzhi and Li, Maolin and Tang, Mingqiu and Han, Shuai and Zhang, Tianning and Zhang, WQ and Luo, Weifeng and others},
  journal={arXiv preprint arXiv:2505.13211},
  year={2025}
}

@article{quevedo2025wpe,
  title={Evaluating Robot Policies in a World Model},
  author={Quevedo, Julian and Liang, Percy and Yang, Sherry},
  journal={arXiv preprint arXiv:2506.00613},
  year={2025}
}

@article{assran2025vjepa2,
  title={V-JEPA~2: Self-Supervised Video Models Enable Understanding, Prediction and Planning},
  author={Assran, Mahmoud and Bardes, Adrien and Fan, David and Garrido, Quentin and Howes, Russell and
Komeili, Mojtaba and Muckley, Matthew and Rizvi, Ammar and Roberts, Claire and Sinha, Koustuv and Zholus, Artem and
Arnaud, Sergio and Gejji, Abha and Martin, Ada and Robert Hogan, Francois and Dugas, Daniel and
Bojanowski, Piotr and Khalidov, Vasil and Labatut, Patrick and Massa, Francisco and Szafraniec, Marc and
Krishnakumar, Kapil and Li, Yong and Ma, Xiaodong and Chandar, Sarath and Meier, Franziska and LeCun, Yann and
Rabbat, Michael and Ballas, Nicolas},
  journal={arXiv preprint arXiv:2506.09985},
  year={2025}
}

@article{zhang2025accvideo,
  title={Accvideo: Accelerating video diffusion model with synthetic dataset},
  author={Zhang, Haiyu and Chen, Xinyuan and Wang, Yaohui and Liu, Xihui and Wang, Yunhong and Qiao, Yu},
  journal={arXiv preprint arXiv:2503.19462},
  year={2025}
}

@misc{runway2025gen4,
  author       = {Runway},
  title        = {Introducing Runway Gen-4
Our next-generation series of AI models for media generation and world consistency.},
  year         = {2025},
  howpublished = {\url{https://runwayml.com/research/introducing-runway-gen-4}},
  note         = {Accessed: 2025-06-13}
}

@article{wan2025wan,
  title={Wan: Open and advanced large-scale video generative models},
  author={Wan, Team and Wang, Ang and Ai, Baole and Wen, Bin and Mao, Chaojie and Xie, Chen-Wei and Chen, Di and Yu, Feiwu and Zhao, Haiming and Yang, Jianxiao and others},
  journal={arXiv preprint arXiv:2503.20314},
  year={2025}
}

@article{kong2024hunyuanvideo,
  title={Hunyuanvideo: A systematic framework for large video generative models},
  author={Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and others},
  journal={arXiv preprint arXiv:2412.03603},
  year={2024}
}

@inproceedings{wang2024animatelcm,
  title={Animatelcm: Computation-efficient personalized style video generation without personalized video data},
  author={Wang, Fu-Yun and Huang, Zhaoyang and Bian, Weikang and Shi, Xiaoyu and Sun, Keqiang and Song, Guanglu and Liu, Yu and Li, Hongsheng},
  booktitle={SIGGRAPH},
  year={2024}
}

@inproceedings{chen2024videocrafter2,
  title={Videocrafter2: Overcoming data limitations for high-quality video diffusion models},
  author={Chen, Haoxin and Zhang, Yong and Cun, Xiaodong and Xia, Menghan and Wang, Xintao and Weng, Chao and Shan, Ying},
  booktitle={CVPR},
  year={2024}
}


@article{bao2024vidu,
  title={Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models},
  author={Bao, Fan and Xiang, Chendong and Yue, Gang and He, Guande and Zhu, Hongzhou and Zheng, Kaiwen and Zhao, Min and Liu, Shilong and Wang, Yaole and Zhu, Jun},
  journal={arXiv preprint arXiv:2405.04233},
  year={2024}
}

@article{fan2025vchitect,
  title={Vchitect-2.0: Parallel transformer for scaling up video diffusion models},
  author={Fan, Weichen and Si, Chenyang and Song, Junhao and Yang, Zhenyu and He, Yinan and Zhuo, Long and Huang, Ziqi and Dong, Ziyue and He, Jingwen and Pan, Dongwei and others},
  journal={arXiv preprint arXiv:2501.08453},
  year={2025}
}

@misc{klingai2024kling,
  author       = {KlingAI},
  title        = {Kling},
  year         = {2024},
  howpublished = {\url{https://app.klingai.com/cn/}},
  note         = {Accessed: 2025-06-13}
}

@misc{genmo2024mochi1,
  author       = {genmo},
  title        = {Mochi-1},
  year         = {2024},
  howpublished = {\url{https://www.genmo.ai/blog}},
  note         = {Accessed: 2025-06-13}
}

@article{chi2024eva,
  title={Eva: An embodied world model for future video anticipation},
  author={Chi, Xiaowei and Fan, Chun-Kai and Zhang, Hengyuan and Qi, Xingqun and Zhang, Rongyu and Chen, Anthony and Chan, Chi-min and Xue, Wei and Liu, Qifeng and Zhang, Shanghang and others},
  journal={arXiv preprint arXiv:2410.15461},
  year={2024}
}

@inproceedings{ma2024latte,
  title={Latte: Latent diffusion transformer for video generation},
  author={Ma, Xin and Wang, Yaohui and Jia, Gengyun and Chen, Xinyuan and Liu, Ziwei and Li, Yuan-Fang and Chen, Cunjian and Qiao, Yu},
  booktitle={TMLR},
  year={2025}
}

@misc{maricle25,
  author       = {Meitu AI},
  title        = {MiracleVision V5},
  year         = {2025},
  howpublished = {\url{https://www.miraclevision.com/}},
  note         = {Accessed: 2025-07-02}
}

@misc{luma24,
  author       = {Luma AI},
  title        = {Luma},
  year         = {2024},
  howpublished = {\url{https://lumalabs.ai/dream-machine}},
  note         = {Accessed: 2025-07-02}
}

@misc{pika23,
  author       = {Pika labs},
  title        = {Pika},
  year         = {2023},
  howpublished = {\url{https://pika.art/login}},
  note         = {Accessed: 2025-07-02}
}

@misc{gen223,
  author       = {Runaway},
  title        = {Gen-2},
  year         = {2023},
  howpublished = {\url{https://runwayml.com/product}},
  note         = {Accessed: 2025-07-02}
}


@inproceedings{shen2023mostgan,
  title={Mostgan-v: Video generation with temporal motion styles},
  author={Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{skorokhodov2022stylegan,
  title={Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2},
  author={Skorokhodov, Ivan and Tulyakov, Sergey and Elhoseiny, Mohamed},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{tulyakov2018mocogan,
  title={Mocogan: Decomposing motion and content for video generation},
  author={Tulyakov, Sergey and Liu, Ming-Yu and Yang, Xiaodong and Kautz, Jan},
  booktitle={CVPR},
  year={2018}
}

@article{yuan2025magictime,
  title={Magictime: Time-lapse video generation models as metamorphic simulators},
  author={Yuan, Shenghai and Huang, Jinfa and Shi, Yujun and Xu, Yongqi and Zhu, Ruijie and Lin, Bin and Cheng, Xinhua and Yuan, Li and Luo, Jiebo},
  journal={TPAMI},
  year={2025}
}



@inproceedings{wei2024dreamvideo,
  title={Dreamvideo: Composing your dream videos with customized subject and motion},
  author={Wei, Yujie and Zhang, Shiwei and Qing, Zhiwu and Yuan, Hangjie and Liu, Zhiheng and Liu, Yu and Zhang, Yingya and Zhou, Jingren and Shan, Hongming},
  booktitle={CVPR},
  year={2024}
}

@article{wang2024magicvideov2,
  title={Magicvideo-v2: Multi-stage high-aesthetic video generation},
  author={Wang, Weimin and Liu, Jiawei and Lin, Zhijie and Yan, Jiangqiao and Chen, Shuo and Low, Chetwin and Hoang, Tuyen and Wu, Jie and Liew, Jun Hao and Yan, Hanshu and others},
  journal={arXiv preprint arXiv:2401.04468},
  year={2024}
}

@inproceedings{girdhar2311emu,
  title={Emu video: Factorizing text-to-video generation by explicit image conditioning},
  author={Girdhar, R and Singh, M and Brown, A and Duval, Q and Azadi, S and Rambhatla, SS and Shah, A and Yin, X and Parikh, D and Misra, I},
  booktitle={ECCV},
  year={2024}
}


@article{zhou2024allegro,
  title={Allegro: Open the black box of commercial-level video generation model},
  author={Zhou, Yuan and Wang, Qiuyue and Cai, Yuxuan and Yang, Huan},
  journal={arXiv preprint arXiv:2410.15458},
  year={2024}
}


@article{zhang2025training,
  title={Training-Free Efficient Video Generation via Dynamic Token Carving},
  author={Zhang, Yuechen and Xing, Jinbo and Xia, Bin and Liu, Shaoteng and Peng, Bohao and Tao, Xin and Wan, Pengfei and Lo, Eric and Jia, Jiaya},
  journal={arXiv preprint arXiv:2505.16864},
  year={2025}
}

@article{li2024t2vturbov2,
  title={T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design},
  author={Li, Jiachen and Long, Qian and Zheng, Jian and Gao, Xiaofeng and Piramuthu, Robinson and Chen, Wenhu and Wang, William Yang},
  journal={arXiv preprint arXiv:2410.05677},
  year={2024}
}

@article{jin2024pyramidal,
  title={Pyramidal flow matching for efficient video generative modeling},
  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},
  journal={arXiv preprint arXiv:2410.05954},
  year={2024}
}

@article{wang2024emu3,
  title={Emu3: Next-token prediction is all you need},
  author={Wang, Xinlong and Zhang, Xiaosong and Luo, Zhengxiong and Sun, Quan and Cui, Yufeng and Wang, Jinsheng and Zhang, Fan and Wang, Yueze and Li, Zhen and Yu, Qiying and others},
  journal={arXiv preprint arXiv:2409.18869},
  year={2024}
}


@inproceedings{yu2023magvit,
  title={Magvit: Masked generative video transformer},
  author={Yu, Lijun and Cheng, Yong and Sohn, Kihyuk and Lezama, Jos{\'e} and Zhang, Han and Chang, Huiwen and Hauptmann, Alexander G and Yang, Ming-Hsuan and Hao, Yuan and Essa, Irfan and others},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{sun2024generative,
  title={Generative multimodal models are in-context learners},
  author={Sun, Quan and Cui, Yufeng and Zhang, Xiaosong and Zhang, Fan and Yu, Qiying and Wang, Yueze and Rao, Yongming and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{henschel2025streamingt2v,
  title={Streamingt2v: Consistent, dynamic, and extendable long video generation from text},
  author={Henschel, Roberto and Khachatryan, Levon and Poghosyan, Hayk and Hayrapetyan, Daniil and Tadevosyan, Vahram and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
  booktitle={CVPR},
  year={2025}
}

@article{sun2024dimensionx,
  title={Dimensionx: Create any 3d and 4d scenes from a single image with controllable video diffusion},
  author={Sun, Wenqiang and Chen, Shuo and Liu, Fangfu and Chen, Zilong and Duan, Yueqi and Zhang, Jun and Wang, Yikai},
  journal={arXiv preprint arXiv:2411.04928},
  year={2024}
}

@article{kondratyuk2023videopoet,
  title={Videopoet: A large language model for zero-shot video generation},
  author={Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jos{\'e} and Huang, Jonathan and Schindler, Grant and Hornung, Rachel and Birodkar, Vighnesh and Yan, Jimmy and Chiu, Ming-Chang and others},
  journal={arXiv preprint arXiv:2312.14125},
  year={2023}
}

@article{gao2025seedance,
  title={Seedance 1.0: Exploring the Boundaries of Video Generation Models},
  author={Gao, Yu and Guo, Haoyuan and Hoang, Tuyen and Huang, Weilin and Jiang, Lu and Kong, Fangyuan and Li, Huixia and Li, Jiashi and Li, Liang and Li, Xiaojie and others},
  journal={arXiv preprint arXiv:2506.09113},
  year={2025}
}

@misc{Veo325,
  author       = {Google Deepmind},
  title        = {Veo3},
  year         = {2025},
  howpublished = {\url{https://deepmind.google/models/veo/}},
  note         = {Accessed: 2025-07-02}
}

@inproceedings{zhang2025epona,
  title={Epona: Autoregressive Diffusion World Model for Autonomous Driving},
  author={Zhang, Kaiwen and Tang, Zhenyu and Hu, Xiaotao and Pan, Xingang and Guo, Xiaoyang and Liu, Yuan and Huang, Jingwei and Yuan, Li and Zhang, Qian and Long, Xiao-Xiao and others},
  booktitle={ICCV},
  year={2025}
}


@inproceedings{qiu2023freenoise,
  title={Freenoise: Tuning-free longer video diffusion via noise rescheduling},
  author={Qiu, Haonan and Xia, Menghan and Zhang, Yong and He, Yingqing and Wang, Xintao and Shan, Ying and Liu, Ziwei},
  booktitle={ICLR},
  year={2024}
}

@article{wang2023gen,
  title={Gen-l-video: Multi-text to long video generation via temporal co-denoising},
  author={Wang, Fu-Yun and Chen, Wenshuo and Song, Guanglu and Ye, Han-Jia and Liu, Yu and Li, Hongsheng},
  journal={arXiv preprint arXiv:2305.18264},
  year={2023}
}

@inproceedings{wang2024zola,
  title={ZoLA: Zero-Shot Creative Long Animation Generation with Short Video Model},
  author={Wang, Fu-Yun and Huang, Zhaoyang and Ma, Qiang and Song, Guanglu and Lu, Xudong and Bian, Weikang and Li, Yijin and Liu, Yu and Li, Hongsheng},
  booktitle={ECCV},
  year={2024}
}

@article{wang2024loong,
  title={Loong: Generating minute-level long videos with autoregressive language models},
  author={Wang, Yuqing and Xiong, Tianwei and Zhou, Daquan and Lin, Zhijie and Zhao, Yang and Kang, Bingyi and Feng, Jiashi and Liu, Xihui},
  journal={arXiv preprint arXiv:2410.02757},
  year={2024}
}

@inproceedings{liu2025langscene,
  title={LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion},
  author={Liu, Fangfu and Li, Hao and Chi, Jiawei and Wang, Hanyang and Yang, Minghui and Wang, Fudong and Duan, Yueqi},
  booktitle={ICCV},
  year={2025}
}

@article{xu2024easyanimate,
  title={Easyanimate: A high-performance long video generation method based on transformer architecture},
  author={Xu, Jiaqi and Zou, Xinyi and Huang, Kunzhe and Chen, Yunkuo and Liu, Bo and Cheng, MengLi and Shi, Xing and Huang, Jun},
  journal={arXiv preprint arXiv:2405.18991},
  year={2024}
}

@misc{jimengai24jimeng,
  author       = {ByteDance},
  title        = {Jimeng},
  year         = {2024},
  howpublished = {\url{https://jimeng-ai.org/}},
  note         = {Accessed: 2025-07-02}
}

@inproceedings{ju2024miradata,
  title={Miradata: A large-scale video dataset with long durations and structured captions},
  author={Ju, Xuan and Gao, Yiming and Zhang, Zhaoyang and Yuan, Ziyang and Wang, Xintao and Zeng, Ailing and Xiong, Yu and Xu, Qiang and Shan, Ying},
  booktitle={NeurIPS},
  year={2024}
}

@article{hacohen2024ltxvideo,
  title={Ltx-video: Realtime video latent diffusion},
  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and others},
  journal={arXiv preprint arXiv:2501.00103},
  year={2024}
}

@misc{jvcvt2v,
  author       = {},
  title        = {JV-CV},
  year         = {2024},
  howpublished = {\url{https://jiutiancv.github.io/JV-CV-T2V/}},
  note         = {Accessed: 2025-07-02}
}

@inproceedings{ma2024sit,
  title={Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers},
  author={Ma, Nanye and Goldstein, Mark and Albergo, Michael S and Boffi, Nicholas M and Vanden-Eijnden, Eric and Xie, Saining},
  booktitle={ECCV},
  year={2024}
}

@article{zhang2024show1,
  title={Show-1: Marrying pixel and latent diffusion models for text-to-video generation},
  author={Zhang, David Junhao and Wu, Jay Zhangjie and Liu, Jia-Wei and Zhao, Rui and Ran, Lingmin and Gu, Yuchao and Gao, Difei and Shou, Mike Zheng},
  journal={In IJCV},
  year={2024}
}

@article{li2023videogen,
  title={Videogen: A reference-guided latent diffusion approach for high definition text-to-video generation},
  author={Li, Xin and Chu, Wenqing and Wu, Ye and Yuan, Weihang and Liu, Fanglong and Zhang, Qi and Li, Fu and Feng, Haocheng and Ding, Errui and Wang, Jingdong},
  journal={arXiv preprint arXiv:2309.00398},
  year={2023}
}

@article{Ho22imagenvideo,
  title={Imagen Video: High Definition Video Generation with Diffusion Models},
  author={Jonathan, Ho and William, Chan and Chitwan, Saharia and Jay, Whang and Ruiqi, Gao and Alexey, Gritsenko and Diederik, P. Kingma and Ben, Poole and Mohammad, Norouzi and David, J. Fleet and Tim, Salimans},
  journal={arXiv preprint arXiv:2210.02303},
  year={2022}
}

@inproceedings{yin2025causvid,
  title={From slow bidirectional to fast autoregressive video diffusion models},
  author={Yin, Tianwei and Zhang, Qiang and Zhang, Richard and Freeman, William T and Durand, Fredo and Shechtman, Eli and Huang, Xun},
  booktitle={CVPR},
  year={2025}
}

@article{lin2024stiv,
  title={STIV: Scalable Text and Image Conditioned Video Generation},
  author={Lin, Zongyu and Liu, Wei and Chen, Chen and Lu, Jiasen and Hu, Wenze and Fu, Tsu-Jui and Allardice, Jesse and Lai, Zhengfeng and Song, Liangchen and Zhang, Bowen and others},
  journal={arXiv preprint arXiv:2412.07730},
  year={2024}
}

@misc{hailuo24minmax,
  author       = {Hailuo AI},
  title        = {Hailuo AI},
  year         = {2024},
  howpublished = {\url{https://hailuoai.video/}},
  note         = {Accessed: 2025-07-02}
}

@misc{hailuo0225minmax,
  author       = {Hailuo AI},
  title        = {Hailuo 02},
  year         = {2025},
  howpublished = {\url{https://hailuoai.video/}},
  note         = {Accessed: 2025-07-02}
}

@inproceedings{menapace2024snapvideo,
  title={Snap video: Scaled spatiotemporal transformers for text-to-video synthesis},
  author={Menapace, Willi and Siarohin, Aliaksandr and Skorokhodov, Ivan and Deyneka, Ekaterina and Chen, Tsai-Shien and Kag, Anil and Fang, Yuwei and Stoliar, Aleksei and Ricci, Elisa and Ren, Jian and others},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{qing2024higen,
  title={Hierarchical spatio-temporal decoupling for text-to-video generation},
  author={Qing, Zhiwu and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Wei, Yujie and Zhang, Yingya and Gao, Changxin and Sang, Nong},
  booktitle={CVPR},
  year={2024}
}
@inproceedings{yu2023magvitv2,
  title={Language Model Beats Diffusion--Tokenizer is Key to Visual Generation},
  author={Yu, Lijun and Lezama, Jos{\'e} and Gundavarapu, Nitesh B and Versari, Luca and Sohn, Kihyuk and Minnen, David and Cheng, Yong and Birodkar, Vighnesh and Gupta, Agrim and Gu, Xiuye and others},
  booktitle={ICLR},
  year={2024}
}

@article{zhou2022magicvideo,
  title={Magicvideo: Efficient video generation with latent diffusion models},
  author={Zhou, Daquan and Wang, Weimin and Yan, Hanshu and Lv, Weiwei and Zhu, Yizhe and Feng, Jiashi},
  journal={arXiv preprint arXiv:2211.11018},
  year={2022}
}

@misc{gen123runaway,
  author       = {Runaway},
  title        = {Gen-1},
  year         = {2023},
  howpublished = {\url{https://runwayml.com/research/gen-1}},
  note         = {Accessed: 2025-07-02}
}

@article{singer2022makeavideo,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@misc{nova25amazonaws,
  author       = {Amazon AWS},
  title        = {Nova Reel},
  year         = {2025},
  howpublished = {\url{https://aws.amazon.com/ai/generative-ai/nova/}},
  note         = {Accessed: 2025-07-02}
}

@article{si2025repvideo,
  title={RepVideo: Rethinking Cross-Layer Representation for Video Generation},
  author={Si, Chenyang and Fan, Weichen and Lv, Zhengyao and Huang, Ziqi and Qiao, Yu and Liu, Ziwei},
  journal={arXiv preprint arXiv:2501.08994},
  year={2025}
}

@article{yi2025magic1for1,
  title={Magic 1-For-1: Generating One Minute Video Clips within One Minute},
  author={Yi, Hongwei and Shao, Shitong and Ye, Tian and Zhao, Jiantong and Yin, Qingyu and Lingelbach, Michael and Yuan, Li and Tian, Yonghong and Xie, Enze and Zhou, Daquan},
  journal={arXiv preprint arXiv:2502.07701},
  year={2025}
}

@misc{stepvideo25jieyue,
  author       = {StepFun},
  title        = {Step-Video-T2V},
  year         = {2025},
  howpublished = {\url{https://yuewen.cn/videos}},
  note         = {Accessed: 2025-07-02}
}

@article{peng2025opensora2,
  title={Open-sora 2.0: Training a commercial-level video generation model in 200 k},
  author={Peng, Xiangyu and Zheng, Zangwei and Shen, Chenhui and Young, Tom and Guo, Xinying and Wang, Binluo and Xu, Hang and Liu, Hongxin and Jiang, Mingyan and Li, Wenjun and others},
  journal={arXiv preprint arXiv:2503.09642},
  year={2025}
}

@misc{MiracleVision25meitu,
  author       = {Meitu AI},
  title        = {MiracleVision},
  year         = {2025},
  howpublished = {\url{https://www.miraclevision.com/}},
  note         = {Accessed: 2025-07-02}
}

@misc{huang2025towardsvwm,
    title={Towards Video World Models},
    author={Huang, Xun},
    year={2025},
    url={https://www.xunhuang.me/blogs/world_model.html}
}




@misc{lecun2022wm,
    title={Yann LeCun on a vision to make AI systems learn and reason like animals and humans},
    author={Yann, LeCun},
    year={2022},
    url={https://ai.meta.com/blog/yann-lecun-advances-in-ai-research/}
}

@misc{mirage2025,
title     = {MirageLSD: Zero-Latency, Real-Time, Infinite Video Generation},
author    = {Decart AI},
year      = {2025},
note      = {Technical Report},
howpublished = {\url{https://mirage.decart.ai/}}}

@inproceedings{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={ICLR},
  year={2022}
}

@article{li2025hunyuangamecraft,
  title={Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition},
  author={Li, Jiaqi and Tang, Junshu and Xu, Zhiyong and Wu, Longhuang and Zhou, Yuan and Shao, Shuai and Yu, Tianbao and Cao, Zhiguo and Lu, Qinglin},
  journal={arXiv preprint arXiv:2506.17201},
  year={2025}
}

@article{huang2025selfforcing,
  title={Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion},
  author={Huang, Xun and Li, Zhengqi and He, Guande and Zhou, Mingyuan and Shechtman, Eli},
  journal={arXiv preprint arXiv:2506.08009},
  year={2025}
}

@article{lin2025apt2,
  title={Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation},
  author={Lin, Shanchuan and Yang, Ceyuan and He, Hao and Jiang, Jianwen and Ren, Yuxi and Xia, Xin and Zhao, Yang and Xiao, Xuefeng and Jiang, Lu},
  journal={arXiv preprint arXiv:2506.09350},
  year={2025}
}

@article{chen2025skyreels,
  title={Skyreels-v2: Infinite-length film generative model},
  author={Chen, Guibin and Lin, Dixuan and Yang, Jiangping and Lin, Chunze and Zhu, Junchen and Fan, Mingyuan and Zhang, Hao and Chen, Sheng and Chen, Zheng and Ma, Chengcheng and others},
  journal={arXiv preprint arXiv:2504.13074},
  year={2025}
}


@article{ma2025stepvideo,
  title={Step-video-t2v technical report: The practice, challenges, and future of video foundation model},
  author={Ma, Guoqing and Huang, Haoyang and Yan, Kun and Chen, Liangyu and Duan, Nan and Yin, Shengming and Wan, Changyi and Ming, Ranchen and Song, Xiaoniu and Chen, Xing and others},
  journal={arXiv preprint arXiv:2502.10248},
  year={2025}
}

@misc{pixverse2025,
title     = {PixVerse},
author    = {AIsphere},
year      = {2025},
howpublished = {\url{https://app.pixverse.ai/home}}}

@inproceedings{chen2024diffusionforcing,
  title={Diffusion forcing: Next-token prediction meets full-sequence diffusion},
  author={Chen, Boyuan and Mart{\'\i} Mons{\'o}, Diego and Du, Yilun and Simchowitz, Max and Tedrake, Russ and Sitzmann, Vincent},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{parmar2018imagegeneration,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle={ICML},
  year={2018}
}

@inproceedings{razavi2019vqvae2,
  title={Generating diverse high-fidelity images with vq-vae-2},
  author={Razavi, Ali and Van den Oord, Aaron and Vinyals, Oriol},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{ramesh2021zeroshott2i,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={ICML},
  year={2021}
}

@article{girdhar2023emuvideo,
  title={Emu video: Factorizing text-to-video generation by explicit image conditioning},
  author={Girdhar, Rohit and Singh, Mannat and Brown, Andrew and Duval, Quentin and Azadi, Samaneh and Rambhatla, Sai Saketh and Shah, Akbar and Yin, Xi and Parikh, Devi and Misra, Ishan},
  journal={arXiv preprint arXiv:2311.10709},
  year={2023}
}


@inproceedings{yuan2024instructvideo,
  title={Instructvideo: Instructing video diffusion models with human feedback},
  author={Yuan, Hangjie and Zhang, Shiwei and Wang, Xiang and Wei, Yujie and Feng, Tao and Pan, Yining and Zhang, Yingya and Liu, Ziwei and Albanie, Samuel and Ni, Dong},
  booktitle={CVPR},
  year={2024}
}

@article{polyak2024moviegen,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{park2025zero4d,
  title={Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion},
  author={Park, Jangho and Kwon, Taesung and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2503.22622},
  year={2025}
}

@article{ren2025cosmosdrivedream,
  title={Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models},
  author={Ren, Xuanchi and Lu, Yifan and Cao, Tianshi and Gao, Ruiyuan and Huang, Shengyu and Sabour, Amirmojtaba and Shen, Tianchang and Pfaff, Tobias and Wu, Jay Zhangjie and Chen, Runjian and others},
  journal={arXiv preprint arXiv:2506.09042},
  year={2025}
}

@article{song2020denoising,
  title={Denoising diffusion implicit models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={arXiv preprint arXiv:2010.02502},
  year={2020}
}

@article{zhou2025inductive,
  title={Inductive moment matching},
  author={Zhou, Linqi and Ermon, Stefano and Song, Jiaming},
  journal={arXiv preprint arXiv:2503.07565},
  year={2025}
}

@article{song2025ideas,
  title={Ideas in inference-time scaling can benefit generative pre-training algorithms},
  author={Song, Jiaming and Zhou, Linqi},
  journal={arXiv preprint arXiv:2503.07154},
  year={2025}
}

@inproceedings{dang2025personalized,
  title={Personalized Preference Fine-tuning of Diffusion Models},
  author={Dang, Meihua and Singh, Anikait and Zhou, Linqi and Ermon, Stefano and Song, Jiaming},
  booktitle={CVPR},
  year={2025}
}

@article{atzmon2024edifyimagediff,
  title={Edify image: High-quality image generation with pixel space laplacian diffusion models},
  author={Atzmon, Yuval and Bala, Maciej and Balaji, Yogesh and Cai, Tiffany and Cui, Yin and Fan, Jiaojiao and Ge, Yunhao and Gururani, Siddharth and Huffman, Jacob and Isaac, Ronald and others},
  journal={arXiv preprint arXiv:2411.07126},
  year={2024}
}

@inproceedings{hatamizadeh2024diffit,
  title={Diffit: Diffusion vision transformers for image generation},
  author={Hatamizadeh, Ali and Song, Jiaming and Liu, Guilin and Kautz, Jan and Vahdat, Arash},
  booktitle={ECCV},
  year={2024}
}

@article{xu2024agg,
  title={Agg: Amortized generative 3d gaussians for single image to 3d},
  author={Xu, Dejia and Yuan, Ye and Mardani, Morteza and Liu, Sifei and Song, Jiaming and Wang, Zhangyang and Vahdat, Arash},
  journal={arXiv preprint arXiv:2401.04099},
  year={2024}
}
@inproceedings{ozturkler2023smrd,
  title={Smrd: Sure-based robust mri reconstruction with diffusion models},
  author={Ozturkler, Batu and Liu, Chao and Eckart, Benjamin and Mardani, Morteza and Song, Jiaming and Kautz, Jan},
  booktitle={MICCAI},

  year={2023}
}

@article{zhang2023improved,
  title={Improved order analysis and design of exponential integrator for diffusion models sampling},
  author={Zhang, Qinsheng and Song, Jiaming and Chen, Yongxin},
  journal={arXiv preprint arXiv:2308.02157},
  year={2023}
}

@inproceedings{song2023lossguided,
  title={Loss-guided diffusion models for plug-and-play controllable generation},
  author={Song, Jiaming and Zhang, Qinsheng and Yin, Hongxu and Mardani, Morteza and Liu, Ming-Yu and Kautz, Jan and Chen, Yongxin and Vahdat, Arash},
  booktitle={ICML},
  year={2023}
}

@inproceedings{zhang2023diffcollage,
  title={Diffcollage: Parallel generation of large content with diffusion models},
  author={Zhang, Qinsheng and Song, Jiaming and Huang, Xun and Chen, Yongxin and Liu, Ming-Yu},
  booktitle={CVPR},
  year={2023}
}

@article{mardani2023variational,
  title={A variational perspective on solving inverse problems with diffusion models},
  author={Mardani, Morteza and Song, Jiaming and Kautz, Jan and Vahdat, Arash},
  journal={arXiv preprint arXiv:2305.04391},
  year={2023}
}

@inproceedings{song2023pseudoinverse,
  title={Pseudoinverse-guided diffusion models for inverse problems},
  author={Song, Jiaming and Vahdat, Arash and Mardani, Morteza and Kautz, Jan},
  booktitle={ICLR},
  year={2023}
}

@article{gu2023seer,
  title={Seer: Language instructed video prediction with latent diffusion models},
  author={Gu, Xianfan and Wen, Chuan and Ye, Weirui and Song, Jiaming and Gao, Yang},
  journal={arXiv preprint arXiv:2303.14897},
  year={2023}
}

@article{lim2023score,
  title={Score-based diffusion models in function space},
  author={Lim, Jae Hyun and Kovachki, Nikola B and Baptista, Ricardo and Beckham, Christopher and Azizzadenesheli, Kamyar and Kossaifi, Jean and Voleti, Vikram and Song, Jiaming and Kreis, Karsten and Kautz, Jan and others},
  journal={arXiv preprint arXiv:2302.07400},
  year={2023}
}

@inproceedings{ye2023affordance,
  title={Affordance diffusion: Synthesizing hand-object interactions},
  author={Ye, Yufei and Li, Xueting and Gupta, Abhinav and De Mello, Shalini and Birchfield, Stan and Song, Jiaming and Tulsiani, Shubham and Liu, Sifei},
  booktitle={CVPR},
  year={2023}
}


@inproceedings{yuan2023physdiff,
  title={Physdiff: Physics-guided human motion diffusion model},
  author={Yuan, Ye and Song, Jiaming and Iqbal, Umar and Vahdat, Arash and Kautz, Jan},
  booktitle={ICCV},
  year={2023}
}

@article{balaji2022ediff,
  title={ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers},
  author={Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Zhang, Qinsheng and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and others},
  journal={arXiv preprint arXiv:2211.01324},
  year={2022}
}

@inproceedings{tashiro2021csdi,
  title={Csdi: Conditional score-based diffusion models for probabilistic time series imputation},
  author={Tashiro, Yusuke and Song, Jiaming and Song, Yang and Ermon, Stefano},
  booktitle={NeurIPS},
  year={2021}
}
@inproceedings{sinha2021d2c,
  title={D2c: Diffusion-decoding models for few-shot conditional generation},
  author={Sinha, Abhishek and Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle={NeurIPS},
  year={2021}
}

@article{meng2021improvedar,
  title={Improved autoregressive modeling with distribution smoothing},
  author={Meng, Chenlin and Song, Jiaming and Song, Yang and Zhao, Shengjia and Ermon, Stefano},
  journal={arXiv preprint arXiv:2103.15089},
  year={2021}
}

@article{meng2021sdedit,
  title={Sdedit: Guided image synthesis and editing with stochastic differential equations},
  author={Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  journal={arXiv preprint arXiv:2108.01073},
  year={2021}
}
@inproceedings{song2020ddim,
  title={Denoising diffusion implicit models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle={ICLR},
  year={2021}
}


@inproceedings{meng2020autoregressive,
  title={Autoregressive score matching},
  author={Meng, Chenlin and Yu, Lantao and Song, Yang and Song, Jiaming and Ermon, Stefano},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{zhao2019infovae,
  title={Infovae: Balancing learning and inference in variational autoencoders},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  booktitle={AAAI},
  year={2019}
}

@misc{lumagenie2025,
title     = {Genie},
author    = {Luma AI},
year      = {2025},
howpublished = {\url{https://lumalabs.ai/genie?view=create}}}


@inproceedings{zhao2017learning,
  title={Learning hierarchical features from deep generative models},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  booktitle={ICML},
  year={2017}
}

@article{zhao2017towards,
  title={Towards deeper understanding of variational autoencoding models},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  journal={arXiv preprint arXiv:1702.08658},
  year={2017}
}

@article{yang2024video,
  title={Video as the new language for real-world decision making},
  author={Yang, Sherry and Walker, Jacob and Parker-Holder, Jack and Du, Yilun and Bruce, Jake and Barreto, Andre and Abbeel, Pieter and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2402.17139},
  year={2024}
}


@inproceedings{kessler2023effectiveness,
  title={The effectiveness of world models for continual reinforcement learning},
  author={Kessler, Samuel and Ostaszewski, Mateusz and Bortkiewicz, Micha{\l}Pawe{\l} and {\.Z}arski, Mateusz and Wolczyk, Maciej and Parker-Holder, Jack and Roberts, Stephen J and Mi, Piotr and others},
  booktitle={CoLLA},
  year={2023}
}

@inproceedings{xu2022learning,
  title={Learning general world models in a handful of reward-free deployments},
  author={Xu, Yingchen and Parker-Holder, Jack and Pacchiano, Aldo and Ball, Philip and Rybkin, Oleh and Roberts, Stephen and Rockt{\"a}schel, Tim and Grefenstette, Edward},
  booktitle={NeurIPS},
  year={2022}
}

@article{lu2022challenges,
  title={Challenges and opportunities in offline reinforcement learning from visual observations},
  author={Lu, Cong and Ball, Philip J and Rudner, Tim GJ and Parker-Holder, Jack and Osborne, Michael A and Teh, Yee Whye},
  journal={arXiv preprint arXiv:2206.04779},
  year={2022}
}


@inproceedings{ball2021augmented,
  title={Augmented world models facilitate zero-shot dynamics generalization from a single offline environment},
  author={Ball, Philip J and Lu, Cong and Parker-Holder, Jack and Roberts, Stephen},
  booktitle={ICML},
  year={2021}
}


@article{liao2025genieEnvisioner,
  title={Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation},
  author={Liao, Yue and Zhou, Pengfei and Huang, Siyuan and Yang, Donglin and Chen, Shengcong and Jiang, Yuxin and Hu, Yue and Cai, Jingbin and Liu, Si and Luo, Jianlan and others},
  journal={arXiv preprint arXiv:2508.05635},
  year={2025}
}

@article{zhu2025asdm,
  title={A-SDM: Accelerating Stable Diffusion through Model Assembly and Feature Inheritance Strategies},
  author={Zhu, Jinchao and Wang, Yuxuan and Pan, Siyuan and Wan, Pengfei and Zhang, Di and Huang, Gao},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2025}
}

@article{wu2025vmoba,
  title={VMoBA: Mixture-of-Block Attention for Video Diffusion Models},
  author={Wu, Jianzong and Hou, Liang and Yang, Haotian and Tao, Xin and Tian, Ye and Wan, Pengfei and Zhang, Di and Tong, Yunhai},
  journal={arXiv preprint arXiv:2506.23858},
  year={2025}
}

@article{luo2025camclonemaster,
  title={CamCloneMaster: Enabling Reference-based Camera Control for Video Generation},
  author={Luo, Yawen and Bai, Jianhong and Shi, Xiaoyu and Xia, Menghan and Wang, Xintao and Wan, Pengfei and Zhang, Di and Gai, Kun and Xue, Tianfan},
  journal={arXiv preprint arXiv:2506.03140},
  year={2025}
}

@article{yu2025contextasmemory,
  title={Context as memory: Scene-consistent interactive long video generation with memory retrieval},
  author={Yu, Jiwen and Bai, Jianhong and Qin, Yiran and Liu, Quande and Wang, Xintao and Wan, Pengfei and Zhang, Di and Liu, Xihui},
  journal={arXiv preprint arXiv:2506.03141},
  year={2025}
}

@article{fu2025robomaster,
  title={Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control},
  author={Fu, Xiao and Wang, Xintao and Liu, Xian and Bai, Jianhong and Xu, Runsen and Wan, Pengfei and Zhang, Di and Lin, Dahua},
  journal={arXiv preprint arXiv:2506.01943},
  year={2025}
}


@article{he2025scaling,
  title={Scaling Image and Video Generation via Test-Time Evolutionary Search},
  author={He, Haoran and Liang, Jiajun and Wang, Xintao and Wan, Pengfei and Zhang, Di and Gai, Kun and Pan, Ling},
  journal={arXiv preprint arXiv:2505.17618},
  year={2025}
}

@article{zhang2025Jenga,
  title={Training-Free Efficient Video Generation via Dynamic Token Carving},
  author={Zhang, Yuechen and Xing, Jinbo and Xia, Bin and Liu, Shaoteng and Peng, Bohao and Tao, Xin and Wan, Pengfei and Lo, Eric and Jia, Jiaya},
  journal={arXiv preprint arXiv:2505.16864},
  year={2025}
}

@article{zhong2025vfrtok,
  title={VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption},
  author={Zhong, Tianxiong and Tian, Xingye and Jiang, Boyuan and Wang, Xuebo and Tao, Xin and Wan, Pengfei and Zhang, Zhiwei},
  journal={arXiv preprint arXiv:2505.12053},
  year={2025}
}


@article{zhang2025motioncrafter,
  title={MotionCrafter: Plug-and-Play Motion Guidance for Diffusion Models},
  author={Zhang, Yuxin and Dong, Weiming and Tang, Fan and Huang, Nisha and Huang, Haibin and Ma, Chongyang and Wan, Pengfei and Lee, Tong-Yee and Xu, Changsheng},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2025}
}


@article{wu2025any2caption,
  title={Any2caption: Interpreting any condition to caption for controllable video generation},
  author={Wu, Shengqiong and Ye, Weicai and Wang, Jiahao and Liu, Quande and Wang, Xintao and Wan, Pengfei and Zhang, Di and Gai, Kun and Yan, Shuicheng and Fei, Hao and others},
  journal={arXiv preprint arXiv:2503.24379},
  year={2025}
}

@article{liu2025rpe,
  title={Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings},
  author={Liu, Cong and Hou, Liang and Zheng, Mingwu and Tao, Xin and Wan, Pengfei and Zhang, Di and Gai, Kun},
  journal={arXiv preprint arXiv:2503.18719},
  year={2025}
}

@article{shi2025diffmoe,
  title={DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers},
  author={Shi, Minglei and Yuan, Ziyang and Yang, Haotian and Wang, Xintao and Zheng, Mingwu and Tao, Xin and Zhao, Wenliang and Zheng, Wenzhao and Zhou, Jie and Lu, Jiwen and others},
  journal={arXiv preprint arXiv:2503.14487},
  year={2025}
}

@article{yang2025efficient,
  title={Efficient Training-Free High-Resolution Synthesis with Energy Rectification in Diffusion Models},
  author={Yang, Zhen and Shen, Guibao and Li, Minyang and Hou, Liang and Liu, Mushui and Wang, Luozhou and Tao, Xin and Wan, Pengfei and Zhang, Di and Chen, Ying-Cong},
  journal={arXiv preprint arXiv:2503.02537},
  year={2025}
}
@article{liu2025improving,
  title={Improving video generation with human feedback},
  author={Liu, Jie and Liu, Gongye and Liang, Jiajun and Yuan, Ziyang and Liu, Xiaokun and Zheng, Mingwu and Wu, Xiele and Wang, Qiulin and Qin, Wenyu and Xia, Menghan and others},
  journal={arXiv preprint arXiv:2501.13918},
  year={2025}
}

@article{bai2024syncammaster,
  title={Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints},
  author={Bai, Jianhong and Xia, Menghan and Wang, Xintao and Yuan, Ziyang and Fu, Xiao and Liu, Zuozhu and Hu, Haoji and Wan, Pengfei and Zhang, Di},
  journal={arXiv preprint arXiv:2412.07760},
  year={2024}
}

@inproceedings{liu2025unleashing,
  title={Unleashing the potential of multi-modal foundation models and video diffusion for 4d dynamic physical scene simulation},
  author={Liu, Zhuoman and Ye, Weicai and Luximon, Yan and Wan, Pengfei and Zhang, Di},
  booktitle={CVPR},
  year={2025}
}

@inproceedings{yin2025towards,
  title={Towards precise scaling laws for video diffusion transformers},
  author={Yin, Yuanyang and Zhao, Yaqi and Zheng, Mingwu and Lin, Ke and Ou, Jiarong and Chen, Rui and Huang, Victor Shea-Jay and Wang, Jiahao and Tao, Xin and Wan, Pengfei and others},
  booktitle={CVPR},
  year={2025}
}

@inproceedings{tian2024videotetris,
  title={Videotetris: Towards compositional text-to-video generation},
  author={Tian, Ye and Yang, Ling and Yang, Haotian and Gao, Yuan and Deng, Yufan and Wang, Xintao and Yu, Zhaochen and Tao, Xin and Wan, Pengfei and ZHANG, Di and others},
  booktitle={NeurIPS},
  year={2024}
}

@article{huang2024owl,
  title={Owl-1: Omni world model for consistent long video generation},
  author={Huang, Yuanhui and Zheng, Wenzhao and Gao, Yuan and Tao, Xin and Wan, Pengfei and Zhang, Di and Zhou, Jie and Lu, Jiwen},
  journal={arXiv preprint arXiv:2412.09600},
  year={2024}
}


@inproceedings{han2024agent,
  title={Agent attention: On the integration of softmax and linear attention},
  author={Han, Dongchen and Ye, Tianzhu and Han, Yizeng and Xia, Zhuofan and Pan, Siyuan and Wan, Pengfei and Song, Shiji and Huang, Gao},
  booktitle={ECCV},
  year={2024}
}


@inproceedings{huang2024placiddreamer,
  title={Placiddreamer: Advancing harmony in text-to-3d generation},
  author={Huang, Shuo and Sun, Shikun and Wang, Zixuan and Qin, Xiaoyu and Xiong, Yanmin and Zhang, Yuan and Wan, Pengfei and Zhang, Di and Jia, Jia},
  booktitle={ACMMM},
  year={2024}
}

@article{yuan20244dynamic,
  title={4dynamic: Text-to-4d generation with hybrid priors},
  author={Yuan, Yu-Jie and Kobbelt, Leif and Liu, Jiwen and Zhang, Yuan and Wan, Pengfei and Lai, Yu-Kun and Gao, Lin},
  journal={arXiv preprint arXiv:2407.12684},
  year={2024}
}
@inproceedings{guo2024i2vadapter,
  title={I2v-adapter: A general image-to-video adapter for diffusion models},
  author={Guo, Xun and Zheng, Mingwu and Hou, Liang and Gao, Yuan and Deng, Yufan and Wan, Pengfei and Zhang, Di and Liu, Yufan and Hu, Weiming and Zha, Zhengjun and others},
  booktitle={SIGGRAPH},
  year={2024}
}

@article{shen2024sgadapter,
  title={Sg-adapter: Enhancing text-to-image generation with scene graph guidance},
  author={Shen, Guibao and Wang, Luozhou and Lin, Jiantao and Ge, Wenhang and Zhang, Chaozhe and Tao, Xin and Zhang, Yuan and Wan, Pengfei and Wang, Zhongyuan and Chen, Guangyong and others},
  journal={arXiv preprint arXiv:2405.15321},
  year={2024}
}

@inproceedings{zhang2023automatic,
  title={Automatic Human Scene Interaction through Contact Estimation and Motion Adaptation},
  author={Zhang, Mingrui and Chen, Ming and Zhou, Yan and Chen, Li and Jian, Weihua and Wan, Pengfei},
  booktitle={ACMMM},
  year={2023}
}


@article{menapace2024pgm,
  title={Promptable game models: Text-guided game simulation via masked diffusion models},
  author={Menapace, Willi and Siarohin, Aliaksandr and Lathuili{\`e}re, St{\'e}phane and Achlioptas, Panos and Golyanik, Vladislav and Tulyakov, Sergey and Ricci, Elisa},
  journal={ACM Transactions on Graphics},
  year={2024}
}


@misc{google2025genie3,
title     = {Genie 3},
author    = {Google Deepmind},
year      = {2025},
howpublished = {\url{https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/}}}


@article{han2025reusability,
  title={Reusability report: Exploring the transferability of self-supervised learning models from single-cell to spatial transcriptomics},
  author={Han, Chuangyi and Lin, Senlin and Wang, Zhikang and Cui, Yan and Zou, Qi and Yuan, Zhiyuan},
  journal={Nature Machine Intelligence},
  year={2025}
}
@article{li2024electron,
  title={Electron-density informed effective and reliable de novo molecular design and lead optimization with ED2Mol},
  author={Li, Mingyu and Song, Kun and Zhao, Mingzhu and You, Gengshu and Zhong, Jie and Zhao, Mengxi and Li, Arong and Chen, Yu and Li, Guobin and Kong, Ying and others},
  journal={Nature Machine Intelligence},
  year={2025}
}


@article{duan2025boosting,
  title={Boosting the predictive power of protein representations with a corpus of text annotations},
  author={Duan, Haonan and Skreta, Marta and Cotta, Leonardo and Rajaonson, Ella Miray and Dhawan, Nikita and Aspuru-Guzik, Al{\'a}n and Maddison, Chris J},
  journal={Nature Machine Intelligence},
  year={2025}
}

@article{li2025kolmogorov,
  title={Kolmogorov--Arnold graph neural networks for molecular property prediction},
  author={Li, Longlong and Zhang, Yipeng and Wang, Guanghui and Xia, Kelin},
  journal={Nature Machine Intelligence},
  year={2025}
}

@article{andani2025histopathology,
  title={Histopathology-based protein multiplex generation using deep learning},
  author={Andani, Sonali and Chen, Boqi and Ficek-Pascual, Joanna and Heinke, Simon and Casanova, Ruben and Hild, Bernard Friedrich and Sobottka, Bettina and Bodenmiller, Bernd and Koelzer, Viktor H and others},
  journal={Nature Machine Intelligence},
  year={2025}
}

@article{zhao2025protein,
  title={Protein--peptide docking with a rational and accurate diffusion generative model},
  author={Zhao, Huifeng and Zhang, Odin and Jiang, Dejun and Wu, Zhenxing and Du, Hongyan and Wang, Xiaorui and Zhao, Yihao and Huang, Yuansheng and Ge, Jingxuan and Hou, Tingjun and others},
  journal={Nature Machine Intelligence},
  year={2025}
}

@article{ing2025integrating,
  title={Integrating multimodal cancer data using deep latent variable path modelling},
  author={Ing, Alex and Andrades, Alvaro and Cosenza, Marco Raffaele and Korbel, Jan O},
  journal={Nature Machine Intelligence},
  year={2025}
}

@article{sanders2023biological,
  title={Biological research and self-driving labs in deep space supported by artificial intelligence},
  author={Sanders, Lauren M and Scott, Ryan T and Yang, Jason H and Qutub, Amina Ann and Garcia Martin, Hector and Berrios, Daniel C and Hastings, Jaden JA and Rask, Jon and Mackintosh, Graham and Hoarfrost, Adrienne L and others},
  journal={Nature Machine Intelligence},
  year={2023}
}

@article{scott2023biomonitoring,
  title={Biomonitoring and precision health in deep space supported by artificial intelligence},
  author={Scott, Ryan T and Sanders, Lauren M and Antonsen, Erik L and Hastings, Jaden JA and Park, Seung-min and Mackintosh, Graham and Reynolds, Robert J and Hoarfrost, Adrienne L and Sawyer, Aenor and Greene, Casey S and others},
  journal={Nature Machine Intelligence},
  year={2023}
}


@article{maurizi2025designing,
  title={Designing metamaterials with programmable nonlinear responses and geometric constraints in graph space},
  author={Maurizi, Marco and Xu, Derek and Wang, Yu-Tong and Yao, Desheng and Hahn, David and Oudich, Mourad and Satpati, Anish and Bauchy, Mathieu and Wang, Wei and Sun, Yizhou and others},
  journal={Nature Machine Intelligence},
  year={2025}
}


@article{nazari2025bioinspired,
  title={Bioinspired trajectory modulation for effective slip control in robot manipulation},
  author={Nazari, Kiyanoush and Mandil, Willow and Santello, Marco and Park, Seongjun and Ghalamzan-E, Amir},
  journal={Nature Machine Intelligence},

  year={2025}
}

@article{jung2024untethered,
  title={Untethered soft actuators for soft standalone robotics},
  author={Jung, Yeongju and Kwon, Kangkyu and Lee, Jinwoo and Ko, Seung Hwan},
  journal={Nature Communications},
  year={2024}
}

@article{marcus2024ideal,
  title={The IDEAL framework for surgical robotics: development, comparative evaluation and long-term monitoring},
  author={Marcus, Hani J and Ramirez, Pedro T and Khan, Danyal Z and Layard Horsfall, Hugo and Hanrahan, John G and Williams, Simon C and Beard, David J and Bhat, Rani and Catchpole, Ken and Cook, Andrew and others},
  journal={Nature medicine},
  year={2024}
}

@article{seong2024multifunctional,
  title={Multifunctional magnetic muscles for soft robotics},
  author={Seong, Minho and Sun, Kahyun and Kim, Somi and Kwon, Hyukjoo and Lee, Sang-Woo and Veerla, Sarath Chandra and Kang, Dong Kwan and Kim, Jaeil and Kondaveeti, Stalin and Tawfik, Salah M and others},
  journal={Nature Communications},
  year={2024}
}

@article{dai2024autonomous,
  title={Autonomous mobile robots for exploratory synthetic chemistry},
  author={Dai, Tianwei and Vijayakrishnan, Sriram and Szczypi{\'n}ski, Filip T and Ayme, Jean-Fran{\c{c}}ois and Simaei, Ehsan and Fellowes, Thomas and Clowes, Rob and Kotopanov, Lyubomir and Shields, Caitlin E and Zhou, Zhengxue and others},
  journal={Nature},
  year={2024}
}

@article{feng2024large,
  title={A large-strain and ultrahigh energy density dielectric elastomer for fast moving soft robot},
  author={Feng, Wenwen and Sun, Lin and Jin, Zhekai and Chen, Lili and Liu, Yuncong and Xu, Hao and Wang, Chao},
  journal={Nature Communications},

  year={2024}
}

@article{mao2024magnetic,
  title={Magnetic steering continuum robot for transluminal procedures with programmable shape and functionalities},
  author={Mao, Liyang and Yang, Peng and Tian, Chenyao and Shen, Xingjian and Wang, Feihao and Zhang, Hao and Meng, Xianghe and Xie, Hui},
  journal={Nature communications},
  year={2024}
}

@article{huang2024programmable,
  title={Programmable adhesion and morphing of protein hydrogels for underwater robots},
  author={Huang, Sheng-Chen and Zhu, Ya-Jiao and Huang, Xiao-Ying and Xia, Xiao-Xia and Qian, Zhi-Gang},
  journal={Nature Communications},
  year={2024}
}

@article{mao2024multimodal,
  title={Multimodal tactile sensing fused with vision for dexterous robotic housekeeping},
  author={Mao, Qian and Liao, Zijian and Yuan, Jinfeng and Zhu, Rong},
  journal={Nature Communications},

  year={2024}
}

@article{liu2024evolution,
  title={The evolution of robotics: research and application progress of dental implant robotic systems},
  author={Liu, Chen and Liu, Yuchen and Xie, Rui and Li, Zhiwen and Bai, Shizhu and Zhao, Yimin},
  journal={International Journal of Oral Science},
  year={2024}
}

@article{chen2024magnetic,
  title={A magnetic multi-layer soft robot for on-demand targeted adhesion},
  author={Chen, Ziheng and Wang, Yibin and Chen, Hui and Law, Junhui and Pu, Huayan and Xie, Shaorong and Duan, Feng and Sun, Yu and Liu, Na and Yu, Jiangfan},
  journal={Nature Communications},

  year={2024}
}

@article{xia2024shaping,
  title={Shaping high-performance wearable robots for human motor and sensory reconstruction and enhancement},
  author={Xia, Haisheng and Zhang, Yuchong and Rajabi, Nona and Taleb, Farzaneh and Yang, Qunting and Kragic, Danica and Li, Zhijun},
  journal={Nature Communications},
  year={2024}
}

@article{abdel2024matched,
  title={A matched case-control analysis of autonomous vs human-driven vehicle accidents},
  author={Abdel-Aty, Mohamed and Ding, Shengxuan},
  journal={Nature communications},
  year={2024}
}

@article{liu2024curse,
  title={Curse of rarity for autonomous vehicles},
  author={Liu, Henry X and Feng, Shuo},
  journal={Nature communications},
  year={2024}
}

@article{tan2024knowledge,
  title={Knowledge as a key determinant of public support for autonomous vehicles},
  author={Tan, Hao and Liu, Jiayan and Chen, Cong and Zhao, Xue and Yang, Jialuo and Tang, Chao},
  journal={Scientific Reports},
  year={2024}
}

@article{lee2024effects,
  title={Effects of human disturbances on wildlife behaviour and consequences for predator-prey overlap in Southeast Asia},
  author={Lee, Samuel Xin Tham and Amir, Zachary and Moore, Jonathan H and Gaynor, Kaitlyn M and Luskin, Matthew Scott},
  journal={Nature Communications},
  year={2024}
}

@article{goldberg2024widespread,
  title={Widespread exposure to SARS-CoV-2 in wildlife communities},
  author={Goldberg, Amanda R and Langwig, Kate E and Brown, Katherine L and Marano, Jeffrey M and Rai, Pallavi and King, Kelsie M and Sharp, Amanda K and Ceci, Alessandro and Kailing, Christopher D and Kailing, Macy J and others},
  journal={Nature Communications},
  year={2024}
}

@article{buysse2024detection,
  title={Detection of Anaplasma and Ehrlichia bacteria in humans, wildlife, and ticks in the Amazon rainforest},
  author={Buysse, Marie and Koual, Rachid and Binetruy, Florian and de Thoisy, Benoit and Baudrimont, Xavier and Garnier, St{\'e}phane and Douine, Maylis and Chevillon, Christine and Delsuc, Fr{\'e}d{\'e}ric and Catzeflis, Fran{\c{c}}ois and others},
  journal={Nature Communications},
  year={2024}
}

@article{watt2025parameters,
  title={Parameters for one health genomic surveillance of Escherichia coli from Australia},
  author={Watt, Anne E and Cummins, Max L and Donato, Celeste M and Wirth, Wytamma and Porter, Ashleigh F and Andersson, Patiyan and Donner, Erica and Jennison, Amy V and Seemann, Torsten and others},
  journal={Nature communications},
  year={2025}
}
@article{baker2025dairy,
  title={Dairy cows inoculated with highly pathogenic avian influenza virus H5N1},
  author={Baker, Amy L and Arruda, Bailey and Palmer, Mitchell V and Boggiatto, Paola and Sarlo Davila, Kaitlyn and Buckley, Alexandra and Ciacci Zanella, Giovana and Snyder, Celeste A and Anderson, Tavis K and Hutter, Carl R and others},
  journal={Nature},

  year={2025}
}

@article{burton2024mammal,
  title={Mammal responses to global changes in human activity vary by trophic group and landscape},
  author={Burton, A Cole and Beirne, Christopher and Gaynor, Kaitlyn M and Sun, Catherine and Granados, Alys and Allen, Maximilian L and Alston, Jesse M and Alvarenga, Guilherme C and Calder{\'o}n, Francisco Samuel {\'A}lvarez and Amir, Zachary and others},
  journal={Nature ecology \& evolution},
  year={2024}
}

@misc{earth22025nvidia,
title     = {Earth-2},
author    = {Nvidia},
year      = {2024},
howpublished = {\url{https://www.nvidia.com/en-us/high-performance-computing/earth-2/}}}

@misc{imaginegrok,
  title     = {Imagine v0.9},
  author       = {X AI},
  year         = {2025},
  howpublished = {\url{https://grok.com/imagine}},
  note         = {Accessed: 2025-10-13}
}


@misc{wan22,
  author       = {Wan AI},
  title        = {Wan2.2},
  year         = {2025},
  howpublished = {\url{https://github.com/Wan-Video/Wan2.2}},
  note         = {Accessed: 2025-10-13}
}

@misc{openai2025sora2,
  author       = {OpenAI},
  title        = {Sora 2},
  year         = {2025},
  howpublished = {\url{https://openai.com/zh-Hans-CN/index/sora-2/}},
  note         = {Accessed: 2025-10-13}
}

@misc{luma2025ray3,
  author       = {Luma AI},
  title        = {Ray3},
  year         = {2025},
  howpublished = {\url{https://lumalabs.ai/dream-machine}},
  note         = {Accessed: 2025-10-13}
}

@misc{meta2025vibes,
  author       = {Meta},
  title        = {Vibes},
  year         = {2025},
  howpublished = {\url{https://about.fb.com/news/2025/09/introducing-vibes-ai-videos/}},
  note         = {Accessed: 2025-10-28}
}

@inproceedings{hafner2019planet,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={ICML},
  year={2019}
}

@article{dai2024automated,
  title={Automated creation of digital cousins for robust policy learning},
  author={Dai, Tianyuan and Wong, Josiah and Jiang, Yunfan and Wang, Chen and Gokmen, Cem and Zhang, Ruohan and Wu, Jiajun and Fei-Fei, Li},
  journal={arXiv preprint arXiv:2410.07408},
  year={2024}
}